
%%
%% Conference Paper for IRC'20, November 9-11, Taichung, Taiwan
%% ***
%%

\documentclass[conference]{IEEEtran}

\newcommand{\stt}[1]{{\small\tt #1}} %\small\tt too small here
\newcommand{\powprof}{\stt{powprofiler}}
\newcommand{\figpath}{./figures}
\let\labelindent\relax

\usepackage[inline]{enumitem}
\usepackage{booktabs}
\usepackage{flushend}
\usepackage{tikz}
\usepackage{siunitx}
\sisetup{per-mode = symbol}
\DeclareSIUnit\fps{fps}
\usepackage{todonotes}
\setuptodonotes{inline}
\newcommand{\adam}[2][]{\todo[color=orange!20, #1]{ADAM: #2}}
\newcommand{\hemi}[2][]{\todo[color=green!20, #1]{HEMI: #2}}


%% citation packege
\usepackage{cite}

%% figures package
\usepackage{graphicx}
\graphicspath{{figures/}}
%\DeclareGraphicsExtensions{.pdf,.jpeg,.png}

%% math package
\usepackage[cmex10]{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}

%% pseudocode package
%\usepackage{algorithmic}

%% packages for alignment
%\usepackage{array}
%\usepackage{mdwmath}
%\usepackage{mdwtab}
%\usepackage{eqparbox}

%% packages for subfigures (eventually)
\usepackage[tight,footnotesize]{subfigure}
%S\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
%\usepackage[caption=false,font=footnotesize]{subfig}

%% package for urls
\usepackage{url}

%% correct bad hyphenation here
\hyphenation{analysis}

%% references (generates a bib file for bibtex)
\begin{filecontents}{\jobname.bib}
@article{salami2014uav,
  title={UAV flight experiments applied to the remote sensing of vegetated areas},
  author={Salam{\'\i}, Esther and Barrado, Cristina and Pastor, Enric},
  journal={Remote Sensing},
  volume={6},
  number={11},
  pages={11051--11081},
  year={2014},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@inproceedings{costa2012use,
  title={The use of unmanned aerial vehicles and wireless sensor network in agricultural applications},
  author={Costa, Fausto G and Ueyama, J{\'o} and Braun, Torsten and Pessin, Gustavo and Os{\'o}rio, Fernando S and Vargas, Patr{\'\i}cia A},
  booktitle={2012 IEEE International Geoscience and Remote Sensing Symposium},
  pages={5045--5048},
  year={2012},
  organization={IEEE}
}
@inproceedings{seewald2020mechanical,
  title={Mechanical and Computational Energy Estimation of a Fixed-Wing Drone}, 
  author={Seewald, Adam and Garcia de Marina, Hector and Midtiby, Henrik Skov and Schultz, Ulrik Pagh},
  booktitle={Proceedings of the 2020 Fourth IEEE International Conference on Robotic Computing (IRC)},
  pages={to appear},
  year={2020},
  organization={IEEE} 
}
@inproceedings{saripalli2002vision,
  title={Vision-based autonomous landing of an unmanned aerial vehicle},
  author={Saripalli, Srikanth and Montgomery, James F and Sukhatme, Gaurav S},
  booktitle={Proceedings 2002 IEEE International Conference on Robotics and Automation (Cat. No. 02CH37292)},
  volume={3},
  pages={2799--2804},
  year={2002},
  organization={IEEE}
}

@inproceedings{saripalli2003landing,
  title={Landing on a moving target using an autonomous helicopter},
  author={Saripalli, Srikanth and Sukhatme, Gaurav S},
  booktitle={Field and service robotics},
  pages={277--286},
  year={2003},
  organization={Springer}
}

@inproceedings{lee2012autonomous,
  title={Autonomous landing of a VTOL UAV on a moving platform using image-based visual servoing},
  author={Lee, Daewon and Ryan, Tyler and Kim, H Jin},
  booktitle={2012 IEEE international conference on robotics and automation},
  pages={971--976},
  year={2012},
  organization={IEEE}
}

@inproceedings{chen2016system,
  title={System integration of a vision-guided UAV for autonomous landing on moving platform},
  author={Chen, Xudong and Phang, Swee King and Shan, Mo and Chen, Ben M},
  booktitle={2016 12th IEEE International Conference on Control and Automation (ICCA)},
  pages={761--766},
  year={2016},
  organization={IEEE}
}
@inproceedings{falanga2017vision,
  title={Vision-based autonomous quadrotor landing on a moving platform},
  author={Falanga, Davide and Zanchettin, Alessio and Simovic, Alessandro and Delmerico, Jeffrey and Scaramuzza, Davide},
  booktitle={2017 IEEE International Symposium on Safety, Security and Rescue Robotics (SSRR)},
  pages={200--207},
  year={2017},
  organization={IEEE}
}	
@article{araar2017vision,
  title={Vision based autonomous landing of multirotor UAV on moving platform},
  author={Araar, Oualid and Aouf, Nabil and Vitanov, Ivan},
  journal={Journal of Intelligent \& Robotic Systems},
  volume={85},
  number={2},
  pages={369--384},
  year={2017},
  publisher={Springer}
}
@article{feng2018autonomous,
  title={Autonomous landing of a UAV on a moving platform using model predictive control},
  author={Feng, Yi and Zhang, Cong and Baek, Stanley and Rawashdeh, Samir and Mohammadi, Alireza},
  journal={Drones},
  volume={2},
  number={4},
  pages={34},
  year={2018},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@article{kyristsis2016towards,
  title={Towards autonomous modular UAV missions: The detection, geo-location and landing paradigm},
  author={Kyristsis, Sarantis and Antonopoulos, Angelos and Chanialakis, Theofilos and Stefanakis, Emmanouel and Linardos, Christos and Tripolitsiotis, Achilles and Partsinevelos, Panagiotis},
  journal={Sensors},
  volume={16},
  number={11},
  pages={1844},
  year={2016},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@inproceedings{acuna2018vision,
  title={Vision-based UAV landing on a moving platform in GPS denied environments using motion prediction},
  author={Acuna, Raul and Zhang, Ding and Willert, Volker},
  booktitle={2018 Latin American Robotic Symposium, 2018 Brazilian Symposium on Robotics (SBR) and 2018 Workshop on Robotics in Education (WRE)},
  pages={515--521},
  year={2018},
  organization={IEEE}
}
@inproceedings{lee2016vision,
  title={Vision-based UAV landing on the moving vehicle},
  author={Lee, Hanseob and Jung, Seokwoo and Shim, David Hyunchul},
  booktitle={2016 International conference on unmanned aircraft systems (ICUAS)},
  pages={1--7},
  year={2016},
  organization={IEEE}
}
@article{nguyen2018lightdenseyolo,
  title={LightDenseYOLO: A fast and accurate marker tracker for autonomous UAV landing by visible light camera sensor on drone},
  author={Nguyen, Phong Ha and Arsalan, Muhammad and Koo, Ja Hyung and Naqvi, Rizwan Ali and Truong, Noi Quang and Park, Kang Ryoung},
  journal={Sensors},
  volume={18},
  number={6},
  pages={1703},
  year={2018},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@article{guo2020precision,
  title={Precision Landing Test and Simulation of the Agricultural UAV on Apron},
  author={Guo, Yangyang and Guo, Jiaqian and Liu, Chang and Xiong, Hongting and Chai, Lilong and He, Dongjian},
  journal={Sensors},
  volume={20},
  number={12},
  pages={3369},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@article{yuan2018hierarchical,
  title={A hierarchical vision-based localization of rotor unmanned aerial vehicles for autonomous landing},
  author={Yuan, Haiwen and Xiao, Changshi and Xiu, Supu and Zhan, Wenqiang and Ye, Zhenyi and Zhang, Fan and Zhou, Chunhui and Wen, Yuanqiao and Li, Qiliang},
  journal={International Journal of Distributed Sensor Networks},
  volume={14},
  number={9},
  year={2018},
  publisher={SAGE Publications Sage UK: London, England}
}
@article{yang2018hybrid,
  title={Hybrid camera array-based uav auto-landing on moving ugv in gps-denied environment},
  author={Yang, Tao and Ren, Qiang and Zhang, Fangbing and Xie, Bolin and Ren, Hailei and Li, Jing and Zhang, Yanning},
  journal={Remote Sensing},
  volume={10},
  number={11},
  pages={1829},
  year={2018},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@inproceedings{olson2011apriltag,
  title={AprilTag: A robust and flexible visual fiducial system},
  author={Olson, Edwin},
  booktitle={2011 IEEE International Conference on Robotics and Automation},
  pages={3400--3407},
  year={2011},
  organization={IEEE}
}
@article{berenz2012autonomous,
  title={Autonomous battery management for mobile robots based on risk and gain assessment},
  author={Berenz, Vincent and Tanaka, Fumihide and Suzuki, Kenji},
  journal={Artificial Intelligence Review},
  volume={37},
  number={3},
  pages={217--237},
  year={2012},
  publisher={Springer}
}
@inproceedings{mei2005case,
  title={A case study of mobile robot's energy consumption and conservation techniques},
  author={Mei, Yongguo and Lu, Yung-Hsiang and Hu, Y Charlie and Lee, CS George},
  booktitle={ICAR'05. Proceedings., 12th International Conference on Advanced Robotics, 2005.},
  pages={492--497},
  year={2005},
  organization={IEEE}
}
@inproceedings{mei2004energy,
  title={Energy-efficient motion planning for mobile robots},
  author={Mei, Yongguo and Lu, Yung-Hsiang and Hu, Y Charlie and Lee, CS George},
  booktitle={IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA'04. 2004},
  volume={5},
  pages={4344--4349},
  year={2004},
  organization={IEEE}
}
@article{mei2006deployment,
  title={Deployment of mobile robots with energy and timing constraints},
  author={Mei, Yongguo and Lu, Yung-Hsiang and Hu, Yu Charlie and Lee, CS George},
  journal={IEEE Transactions on robotics},
  volume={22},
  number={3},
  pages={507--522},
  year={2006},
  publisher={IEEE}
}
@article{morales2009power,
  title={Power consumption modeling of skid-steer tracked mobile robots on rigid terrain},
  author={Morales, Jesus and Martinez, Jorge L and Mandow, Anthony and Garc{\'\i}a-Cerezo, Alfonso J and Pedraza, Salvador},
  journal={IEEE Transactions on Robotics},
  volume={25},
  number={5},
  pages={1098--1108},
  year={2009},
  publisher={IEEE}
}
@article{seewald2019coarse,
  title={Coarse-Grained Computation-Oriented Energy Modeling for Heterogeneous Parallel Embedded Systems},
  author={Seewald, Adam and Schultz, Ulrik Pagh and Ebeid, Emad and Midtiby, Henrik Skov},
  journal={International Journal of Parallel Programming},
  pages={1--22},
  year={2019},
  publisher={Springer}
}
@inproceedings{nardi2015introducing,
  title={Introducing SLAMBench, a performance and accuracy benchmarking methodology for SLAM},
  author={Nardi, Luigi and Bodin, Bruno and Zia, M Zeeshan and Mawer, John and Nisbet, Andy and Kelly, Paul HJ and Davison, Andrew J and Luj{\'a}n, Mikel and O'Boyle, Michael FP and Riley, Graham and others},
  booktitle={2015 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={5783--5790},
  year={2015},
  organization={IEEE}
}
@article{sadrpour2013mission,
  title={Mission Energy Prediction for Unmanned Ground Vehicles Using Real-time Measurements and Prior Knowledge},
  author={Sadrpour, Amir and Jin, Jionghua and Ulsoy, A Galip},
  journal={Journal of Field Robotics},
  volume={30},
  number={3},
  pages={399--414},
  year={2013},
  publisher={Wiley Online Library}
}
@inproceedings{sadrpour2013experimental,
  title={Experimental validation of mission energy prediction model for unmanned ground vehicles},
  author={Sadrpour, Amir and Jin, Judy and Ulsoy, A Galip},
  booktitle={2013 American Control Conference},
  pages={5960--5965},
  year={2013},
  organization={IEEE}
}

@misc{NfoldMarker,
    title = "{N-fold marker tracker repository}",
    author="{Henrik Skov Midtiby}",
    year = "2015",
    howpublished = "\url{https://github.com/henrikmidtiby/MarkerLocator}"
}

@misc{ArUco_marker,
    title = "{Detection of ArUco markers}",
    author="{OpenCV}",
    year = "2020",
    howpublished = "\url{https://docs.opencv.org/3.4/d5/dae/tutorial_aruco_detection.html}"
}

@misc{Dynamic_pressure_NASA,
    title = "{Dynamic pressure (NASA)}",
    author="{NASA}",
    year = "2020",
    howpublished = "\url{https://www.grc.nasa.gov/WWW/K-12/airplane/dynpress.html}"
}

@book{anderson2010fundamentals,
  title={Fundamentals of aerodynamics},
  author={Anderson Jr, John David},
  year={2010},
  publisher={Tata McGraw-Hill Education}
}

@article{yolov3,
  title={YOLOv3: An Incremental Improvement},
  author={Redmon, Joseph and Farhadi, Ali},
  journal = {arXiv},
  year={2018}
}

@misc{bjelonicYolo2018,
  author = {Marko Bjelonic},
  title = {{YOLO ROS}: Real-Time Object Detection for {ROS}},
  howpublished = {\url{https://github.com/leggedrobotics/darknet_ros}},
  year = {2016--2018},
}

\end{filecontents}

\begin{document}

%% [Ad: typo] ...quadrocopter... -> ...quadcopter... {I would actually use multirotor UAV / not only quadrotors}
%% [Ad: little suggestion] An Energy-Efficient Vision-Based Autonomous Tracking and Landing Approach for a Multirotor UAV on a Moving Platform in an Agricultural Use-Case


% [Ad: Quadrocopter; so it should work as well despite what I said earlier, but I checked IEEExplore and Quadrotor is far more used]
\title{Energy-Sensitive Vision-Based Autonomous Tracking and Landing of a Multirotor on a Moving Platform} 
%\title{Vision-Based Autonomous Tracking and Landing of a Multirotor on a Moving Platform} 
%for an Agricultural Application %UPS: suggested title, agriculture less important, playing with "energy sensitive" since we know about the energy and can adapt QoS accordingly
%UPS: suggesting multirotor, we don't care how many rotors it has...
%\title{Energy-Sensitive Vision-Based Autonomous Tracking and Landing of a Quadrocopter on a Moving Platform} %for an Agricultural Application %UPS: suggested title, agriculture less important, playing with "energy sensitive" since we know about the energy and can adapt QoS accordingly

%% author names and affiliations
\author{
\IEEEauthorblockN{Georgios Zamanakos, Adam Seewald, Henrik Skov Midtiby, and Ulrik Pagh Schultz}
\IEEEauthorblockA{SDU UAS Center, M{\ae}rsk Mc-Kinney M{\o}ller Institute\\
University of Southern Denmark\\
Email: \{*\}@mmmi.sdu.dk
}}

%% make the title area
\maketitle


\begin{abstract}

%\boldmath
abstract\\
abstract\\
abstract\\
abstract\\
abstract\\
abstract\\
abstract\\
abstract\\
abstract\\
abstract\\
abstract\\
abstract\\
abstract\\
abstract

\end{abstract}

% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:introduction}

Unmanned Aerial Vehicles (UAVs) are increasingly used for applications
such as monitoring, surveillance, transportation of small payloads,
and agricultural applications~\cite{costa2012use,salami2014uav}.
%
One of the major constraints of such applications is their limited level of autonomy due to battery limitations. %It is therefore seen that to further 
%
Extending the flying time of a UAV is normally done by having it land
in order to replace or charge the battery before continuing the
mission. Performing landings autonomously can however be challenging
depending on the environment and whether the landing platform is
stationary or mobile.
%
%Apart from presenting an increased energy efficiency concerning the computations, our approach further elaborates on the landing of the UAV on a moving platform. It can be easily shown that it is not generally energy- or time-efficient for the UAV to fly back to its base, once the battery reaches a critical level. Instead it is proposed that the UAV should land on the moving vehicle that is inherently in the proximity of the UAV during normal operation. 
%
Moreover, relying solely on the availability of a GPS signal for
autonomous precision landing is not considered safe, since GPS signals
can be temporarily lost or even tampered with.
%
%The noise and errors of the GPS signal are not predictable and instead a 
%
As an alternative, in this paper we investigate the use of a novel
vision-based autonomous landing system, and evaluate its robustness
towards environmental conditions such as visual disturbances and wind.
%
%is considered a more optimal option.

Extension of the flight time can be also achieved by using
\emph{energy-sensitive algorithms} that can reduce energy consumption
by reducing the quality of service (QoS). With this approach,
energy-costly computations such as computer vision are adapted by
selecting the desired quality of service to match the available
energy~\cite{seewald2020mechanical}.
%
%With such an approach,
%
By combining energy-sensitive algorithms with autonomous landing
capabilities, we aim to increase the total availability of the UAV to
perform operations, by extending the flight time and using autonomous
recharging when needed.
%
%reduce the energy consumption of the UAV during flight and furthermore increase the level of autonomy.

The main contribution of this paper concerns the experimental study of
a robust vision-based algorithm for autonomous
tracking and landing in varying environmental conditions. The
algorithms are executed on an NVIDIA Jetson Nano companion computer
controlling a simulated drone. The vision-based tracking and landing
algorithms provide novel capabilities in terms of tolerance to visual
disturbance and varying environmental conditions such as wind.
%
Our experiments are based on an agricultural use case where a
multirotor UAV performs visual identification of ground-based hazards
while tracking and landing on a moving platform.
%
%will be studied.

%%%%%%%%%%%%%%%%%%%%%%
\section{State of the Art}
\label{sec:state-of-the-art}

Vision-based autonomous landing on a marker has been extensively
studied by many researchers. Key distinctions include whether the
marker is on a moving platform, the type of the marker, the algorithms
used to detect it, as well as the mounted sensors on-board of the UAV.

For stationary platforms, one of the first experiments with
vision-based autonomous landing was conducted by Saripalli
et~al.~\cite{saripalli2002vision}. Here, a helicopter with a color
camera facing vertically towards the ground would land on an ”H”-shape
pattern (similar to ones found on a helipad) using a hierarchical
behavior-based control architecture. In physical tests a marker of
\SI{122}{\cm}$\times$\SI{122}{\cm} size was detected for a maximum altitude of \SI{10}{\m}. A
landing marker inspired by a QR code but consisting of three
artificial markers is demonstrated by Yuan
et~al.~\cite{yuan2018hierarchical}, and was shown to provide a 6-Degree Of Freedom (DOF)
pose over an altitude range of 0-\SI{20}{\m}. Our work is however focused on
the ability to land on moving platforms.

Saripalli et~al.~\cite{saripalli2003landing} also demonstrated the use
of a Kalman Filter to track a moving platform. However all the
computations were performed offline. Similarly, an ArUco marker was
used as a landing marker by Lee et~al.~\cite{lee2012autonomous} to
detect a moving platform. The control of the UAV is performed based on
the error provided by the vision algorithm but all the computations
were performed off-board. Arrar, et~al.~\cite{araar2017vision} focus
on extending the detection range by using an
AprilTag~\cite{olson2011apriltag} as a landing marker. Again all the
computer vision algorithms were also executed off-board. Conversely, a crucial
aspect of our application is to perform all the computations on-board,
and to evaluate them according to their energy efficiency as a
function of QoS.

%\hemi{I think the last sentence mixes what others
%  have done and what we want to do in the paper.}

The design of the marker and choice of sensors can facilitate doing the computations on-board. %Is this the point of this paragraph?
%
Chen et~al.~\cite{chen2016system} utilized a marker consisting of a
circle and rectangles of different colors along with a LiDAR scanning
range finder for height estimation. The marker was detected by
performing color segmentation on the incoming image frame. By fusing
the height measurement from the LiDAR into the vision measurement, a
relative pose of the UAV from the moving platform was obtained. A
color segmentation approach was also implemented by Lee
et~al.~\cite{lee2016vision}. A red rectangle was used as a landing
marker and a vertically facing camera with a fish-eye lens was used to
detect it, and a successful landing from an altitude of \SI{70}{\m} was
demonstrated. Both teams have used an on-board companion computer to
perform all the computation on the UAV. However we in this work do not consider a color segmentation
approach as a safe option, since for a realistic (outdoor)
case it would be difficult, if not impossible, to ensure that the
landing marker will be the only object of a specific color in the
scene.

The use of a hybrid camera system consisting of a fish-eye IR camera
and a stereo camera was demonstrated by Yang
et~al.~\cite{yang2018hybrid}. An ArUco marker was used to mark the
moving platform and a convolutional neural network (CNN) YOLO~v3 was
trained specifically for marker detection. A similar approach
concerning the detection of a landing marker was demonstrated by
Nguyen et~al.~\cite{nguyen2018lightdenseyolo}. Here a specific landing
marker was used and a specific CNN was used to detect it: successful
detection of a \SI{1}{\m}$\times$\SI{1}{\m} marker size was demonstrated from a
distance of \SI{50}{\m}. An AprilTag marker was used as a landing marker by
Kyritsis et~al.~\cite{kyristsis2016towards} for the purpose of ``2016
DJI Developer Challenge''. The identification of the AprilTag marker
was performed through Graphics Processing Unit (GPU). The three teams
have utilized the companion's computer GPU to detect the landing
marker. In the agricultural use case addressed in this paper, the GPU
is however needed for a CNN to detect ground hazards, and since the
GPU cannot simultaneously run different algorithms,
%
%objects around the moving platform and
%
the CPU should be used for detecting the landing marker. By doing so,
a different QoS could be chosen for each algorithm.  

% [Ad: energy-sensitive part {begin}]
%
To account for the energy modeling of computer vision algorithms, we
considered the work previously carried by Nardi
et~al.~\cite{nardi2015introducing}. The authors present SLAMBench, a
framework that investigates Simultaneous Localisation and Mapping (SLAM) algorithms configuration alternatives
for energy efficiency. Whereas, we use \powprof{}, a generic energy
modeling tool~\cite{seewald2019coarse}. This tool enables measuring
the energy impact of different configurations of the ROS-based system
implementing the agricultural use-case. In this paper we present
extensions to \powprof{} that facilitates the initial exploration of
the energy usage of complex, Robot Operative System (ROS)--based robotic systems.

Other approaches to energy modeling, such as the mission-based energy
models studied by Sadrpour et~al.~\cite{sadrpour2013experimental,
  sadrpour2013mission}, focus mostly on ground-based autonomous
vehicles instead of the UAVs. Morales et al.~\cite{morales2009power}
extensively investigated the relation between motion and energy in a
robot, but do not account for on the energy required for computation.
%
Energy modeling of mobile robots as carried by Mei et
al.~\cite{mei2006deployment, mei2005case, mei2004energy} has provided
the ground for the concept of modeling computation for
energy-sensitive algorithm design. Indeed, the approach employed in this paper has evolved
from an energy-efficient motion planning technique
in~\cite{mei2004energy}, a design strategy that allows accounting for
motion and computations separately in~\cite{mei2005case}, to an
energy-efficient deployment algorithm in~\cite{mei2006deployment}.

The battery in our system is considered in the context of a drone
being able to perform its mission while accounting for the eventuality
of a battery shortage; to this end, we investigated the approach
presented by Berenz et~al.~\cite{berenz2012autonomous}, where a
battery management mission-based dynamic recharge approach is
presented. A set of recharge stations are used, along with
self-docking capable robots. Our approach similarly allows landing on
a moving platform for recharging, which is in the context of this
paper is considered in the proximity of the drone. The actual landing
is handled by the proposed algorithm, and we also account for the
energy required for executing this algorithm during landing.
%
% [Ad: energy-sensitive part {end} ]

Taking into account varying environmental conditions and unpredictable
movements of the platform to land on is relevant for the use of
landing in outdoor, mobile scenarios.  Regarding wind conditions, an
AprilTag marker was used by Feng et~al.~\cite{feng2018autonomous} with
a constant wind speed of \SI{5}{\m \per \s} as an external disturbance in a
simulation environment. Nevertheless, a fluctuation in the wind's
magnitude and direction is likely to happen in realistic cases.
%/
Concerning estimation of the moving platform's position and velocity,
similar to our approach a Kalman Filter or Extended Kalman Filter (EKF) has been used for the
estimation~\cite{araar2017vision,feng2018autonomous,
  falanga2017vision}, whereas Yang et~al.~\cite{yang2018hybrid}
constructed a velocity observer algorithm by calculating the actual
moving distance of the moving platform over a period of time.

\section{Energy-Sensitive Mission Deployment} 
\label{sec:approach}

\subsection{Overall approach}

The energy-sensitive design is a mission-oriented concept that adjusts
the computations to the mission being performed while taking into
account energy requirements, including energy consumed by actuation,
computation, and the presence of a limited power source. Specifically,
in the agricultural use-case, the concept is employed to profile and eventually adapt the
computationally heavy algorithms performing autonomous tracking,
landing, and hazard detection. This adaptation enables
energy-sensitivity, in the sense that QoS parameters can be modified
to enable the mission to be completed at the highest possible QoS
level that does not exceed the available energy budget. Tradeoffs
between QoS parameters can be performed by an end-user, i.e., trading
the robustness towards wind during landing for precision of hazard
detection.

The energy-sensitive design using \powprof{} relies on empirical
experiments to measure the actual power consumption on the robot
hardware~\cite{seewald2019coarse}. In this paper we focus on the
initial profiling using of energy usage of the companion computer
(which from the point of view of energy consumption
can be studied independently from the specific drone it is mounted in).

First, the developer specifies the maximum and minimum QoS level for
each algorithm running on the system.  During mission execution the
levels are statically defined: automatic adaptation during different
phases of the mission is being currently investigated and is
considered future work.
%
Then, the developer executes the system to empirically determine the
power consumption. This can be done in two different ways:
%
% [Ad: enumerate?] for some reason I don't think the enumerate env actually ads anything in readability, while I would use it to describe steps of the algorithm, I would just put the following two enumerate as new paragraph with the mode in italic (maybe just me; if others like the enumerate env I am totally fine with it)
\begin{enumerate}
% [Ad: no citation to IRC-2020] we didn't include ROS there, we did for IROS-2020; however the IJPP could work well.
\item Automatically using \powprof{} to control the experiment
  execution~\cite{seewald2019coarse}.  We assume that the algorithms
  are wrapped as ROS nodes, and we require the developer to specify
  the QoS parameters using a ROS configuration.We use a configuration
  file in a key-value pair format which is then interpreted by
  \powprof{}, enabling \powprof{} to iterate through all possible
  combinations and empirically sample the energy consumption of each
  combination of QoS parameters. Once all combinations have been
  iterated through, \powprof{} automatically combines the energy
  consumption data into a complete model.
\item Semi-automatically using \powprof{} to sample energy and combine
  the results of all experiments, but allowing the developer to
  control all aspects of the experiment execution. This approach is
  new and is described in more detail later in this
  section. Basically, a ROS node interfaces to \powprof{} and is used
  by the developer to start/stop sampling in a given
  configuration. Once all experiments have completed, \powprof{} is
  invoked by the developer to combine the energy consumption data into
  a complete model.
\end{enumerate}
%
Regardless of the approach, \powprof{} builds a single model mapping
QoS to total system energy consumption. Coarse-grained sample is
employed to reduce the number of experiments, and missing values are
automatically inferred from the others by the means of a multivariate
linear interpolation.

In the context of this paper, sampling experiments are iterated in a
simulated environment with different configurations.  For example, the
autonomous tracking allows changing the tracking algorithm QoS in
terms of frequency, the landing algorithm in terms of frequency, and
hazard detection QoS in terms of frequency.
%
%frames-per-second (FPS) rate the CNN runs at.

%In the final step, the model is assessed and the QoS is adjusted to the desired granularity of the algorithms, along with the energy requirements.

\subsection{Semi-automatic energy profiling}

%\adam{TODO? Short description of ROS node and combining level 1 models using powprof. Explicit subsection to make clear it's new. Goal is to avoid manual steps such a giving intermediate files magic names or whatever (since this reduces risk of errors). Could show example of what developer code looks like, i.e., what Georgios need to do to use it?}

% [Ad: added description of the semi-automatic mode {begin}]
%
A ROS node has been developed for the purposes of the semi-automatic %energy-sensitive
approach described in this paper. This node allows automatic
generation of the basic energy models that map time to the
instantaneous power consumption. To activate this functionality, the developer simply publishes on a ROS
topic to start the model generation, with \powprof{} accounting for
the invocation of an asynchronous thread which collects data from the
energy sensors. Similarly, the developer publishes on another ROS topic to
stop the model generation, while \powprof{} finalizes collecting data from
sensors, builds the basic energy model, and stores it for later processing.

Once all the basic energy models for the desired QoS ranges have been
collected, a model that maps the energy consumption to QoS
is specified in a configuration file. Here, the developer defines
what QoS configuration corresponds to which basic model
(instantaneous power consumption as a function of time), and runs
\powprof{} using this configuration file as a parameter.
%
% [Ad: ... {end}]

%%%%%%%%%%%%%%%%%%%%
\section{Vision-Based Autonomous Tracking and Landing}
\label{sec:landing}

The vision-based autonomous tracking and landing can be split into
four main sub-problems: detection of the moving platform, navigation,
guidance, and control of the UAV. From an energy-sensitive design
approach, our focus is on the computer vision algorithms used to
detect the moving platform and the parameterization by a QoS
influencing energy consumption and performance, as described in
Section~\ref{sec:Detection}. Furthermore, the navigation block is
designed to increase the robustness and overall performance of the
system, as described in Section~\ref{sec:Navigation}. Last, a model of
dynamically changing wind disturbances is analysed and described in
Section~\ref{sec:WindDisturbances}, allowing the system to be tested
on a realistic use case.


\subsection{Detection of the moving platform}
\label{sec:Detection}

To mark the moving platform a special pattern is constructed,
consisting of an n-fold marker~\cite{NfoldMarker} along with three
ArUco markers~\cite{ArUco_marker} with different ids. This pattern
will be referred to as the \emph{landing marker} and can be seen in
Figure~\ref{fig:LandingMarker}. The n-fold marker is primarily used to
detect the moving platform from a high altitude, while the ArUco
markers are used as extra landmarks in case the marker is partially
visible in the image frame.

To evaluate the computer vision algorithm for detecting the landing
marker, real images, of 640$\times$480 pixel size, were captured with
an Intel RealSense D435 camera. In Gazebo simulation a color camera
with the same distortion coefficients as the Intel camera, is used to
output a 640$\times$480 pixel image at 10 frames per second (fps).

\begin{figure}
\centering
\includegraphics[scale=0.15]{n-fold-for-landing.png}
\caption{The landing marker}
\label{fig:LandingMarker}
\end{figure}

To extract the pixel coordinates of the tip of the n-fold marker, a
kernel size of 13$\times$13 pixels consisting of a real and imaginary
part is created. For every pixel in the image, a convolution is
performed with this kernel and the magnitude of the convolution is
stored. The pixel with the highest magnitude is considered as a
candidate tip of the n-fold marker. For that candidate pixel only, an
estimation of the orientation/phase of the marker is made and an
overall normalized quality score between 0.0 and 1.0 is calculated. If
the score is above a desired threshold value then the pixel is
accepted as the tip of the n-fold marker. If an n-fold marker is
detected, the result will be the pixel coordinates of the tip of the
n-fold marker along with its orientation/phase.

Since a convolution is a computational expensive process, an increase
in the kernel size would also increase the computation time and
therefore the energy consumption. However a higher computation time and energy consumption
is preferred over a non-detection of the n-fold marker. To balance
between energy consumption and effective marker detection, an adaptive
kernel selection function is created to ensure the selection of a
proper kernel size based on a threshold quality score value. In figure
\ref{fig:NfoldOcclusions} an Intel RealSense D435 Depth camera is used
to capture the image and measure also the distance \emph{d} from the n-fold
marker. Based on a desired quality \emph{q}, the proper kernel size \emph{k} is
selected. It is seen that an occlusion on the tip of the n-fold marker
results in a significant increase on the selected kernel size.
\hemi{To me this looks like a significant \emph{decrease} in kernel
  size \ldots}
%
% Georgios: The kernel size, while having an occlusion on the tip of the nfold marker, is 201x201

\begin{figure}[h]
\centering
\includegraphics[scale=0.25]{nfold_occlusions.png}
\caption{Detection of the landing marker under different
  occlusions. On the left an occlusion on the tip of the n-fold marker
  and on the right an occlusion on a sector of the n-fold marker.}
\label{fig:NfoldOcclusions}
\end{figure}

%
To detect the ArUco markers the standard OpenCV library is used and if
any ArUco markers are detected, their central pixel coordinates and
pose are stored.

The next step is to convert these pixel coordinates into a real world
relative position \emph{[X,Y,Z]} according to a local coordinate frame. The
origin of the local coordinate frame \emph{[0,0,0]} is defined as the
center of the landing marker and alignment according to the North,
East, Down (\emph{NED}) frame.
%
%UPS: @Georgios I'm not sure I understand what the NED frame is, can you rephrase or elaborate?
%
The available sensor measurements and sensor fusion algorithms from
the flight controller will be used in this process. The PX4 flight
controller outputs through mavlink messages the altitude of the UAV
and the attitude of the UAV (roll,pitch,yaw). For the \emph{Z} component,
the altitude of the UAV from the flight controller's EKF is used. To
obtain the \emph{X,Y} components, an algorithm is constructed to convert
pixel coordinates into real world \emph{X,Y} coordinates, in meters:
%
%. This algorithm is described below:
%
\begin{enumerate}

\item The pose of the camera in UAV's \emph{BODY} frame is calculated by utilizing the roll and pitch IMU data.

\item The normalized coordinates of the four image corners, according to the camera's horizontal, vertical field of view and the camera's \emph{BODY} pose from step 1, are calculated.

\item The world coordinates of the four image corners are determined by using the normalised coordinates from step 2 along with the UAV's altitude. The result is a projection plane of the image corners on the ground.

\item The perspective homography matrix is calculated between the 2 planes, the image plane and the world plane from step 3.

\item The pixel coordinates are converted into world coordinates by using the homography matrix from step 4.

\item The world coordinates from step 5, are converted from the camera's \emph{BODY} frame into the camera's \emph{NED} frame by using the yaw IMU data from the flight controller.

\item The world coordinates from step 6, are converted from camera's \emph{NED} frame into the landing site's local coordinate frame.

\item For ArUco markers only, an offset vector in the x,y axis is added depending on the distance of each ArUco marker from the tip of the n-fold marker. It is assumed that this vector is prior known.

\end{enumerate}
%
The mean measurements from the detected n-fold and/or ArUco markers
are used to determine the position and orientation of the landing
marker. The result is an \emph{[X,Y,Z]} relative position of the UAV from
the moving platform along with the yaw orientation of the landing
marker.


\subsection{Navigation}
\label{sec:Navigation}

The purpose of the navigation block is to provide an accurate
prediction for the state of the UAV at any given time. This is
important because it will allow us to process images at different fps
according to a desired QoS. Furthermore, the overall robustness of the
system is increased in case the moving platform is not detected in
every image frame. A velocity estimator for the moving platform will
also be implemented as a part of the navigation block.

The variables of interest that describe the state of the UAV for this project are:
%
\begin{itemize}

\item The relative position of the UAV from the moving platform \emph{[X,Y,Z]} calculated as described in Section~\ref{sec:Detection}.

\item The attitude of the UAV [roll, pitch, yaw], obtained from the flight controller's IMUs.

\item the velocity of the UAV \emph{[vx,vy,vz]} in \emph{NED} frame, obtained from the flight controller's EKF.

\item the acceleration of the UAV \emph{[ax,ay,az]} in \emph{NED} frame, obtained either from the flight controller's EKF or by differentiating the velocities.

\end{itemize}
%
The altitude, attitude, velocity and acceleration variables of the
UAV's state are obtained from the flight controller's onboard sensors
and already implemented sensor fusion algorithms (EKF). To fuse those
state variables with the obtained position measurements from
Section~\ref{sec:Detection}, a Kalman Filter will be used. The
measurements from the flight controller will be used in the prediction
step.

Prediction Step:
\begin{equation} 
 \begin{array}{l}
    \hat{x}_{t} = F_{t} * \hat{x}_{t-1} + G_{t} *u_{t} \\ 
    P_{t} = F_{t}* P_{t-1} * F_{t}^\top + Q_{t}
  \end{array}
\end{equation}

where:

\begin{equation*}
\begin{array}{l}


 \hat{x}_{t} = \begin{bmatrix} 
                x_{NED} \\ 
                y_{NED}
                \end{bmatrix} ,  \quad
F_{t} = \begin{bmatrix} 
                1 & 0 \\ 
                0 & 1
                \end{bmatrix}      , \\
G_{t} =   \begin{bmatrix} 
                dt & 0 & \frac{dt^{2}}{2} & 0 & -dt & 0\\ 
                0 & dt & 0 & \frac{dt^{2}}{2} &0 & -dt
                \end{bmatrix} , \quad
u_{t} = \begin{bmatrix} 
                vx_{NED} \\ 
                vy_{NED} \\
                ax_{NED} \\
                ay_{NED} \\
                vx_{m_{NED}}\\
                vy_{m_{NED}}
                \end{bmatrix}  \\ \\
                
P_{t} =  \begin{bmatrix} 
                1 & 0 \\ 
                0 & 1
                \end{bmatrix}   , \quad 
Q_{t}  =\begin{bmatrix} 
                dt & 0 \\ 
                0 & dt
                \end{bmatrix} *\sigma^{2}_{IMU} 
\end{array} \\
\end{equation*}

where:
\begin{itemize}
\item  \(x_{NED}, y_{NED} \) is the position of the UAV in the landing site's local coordinate system (aligned with NED frame), 
\item \(dt\) is the time interval the PX4's EKF outputs the \(vx_{NED},vy_{NED}\) data, usually around 33ms, 
\item \(vx_{NED},vy_{NED}\) is the linear velocity of the UAV in NED frame, estimated from the PX4's EKF,
\item \(ax_{NED}, ay_{NED}\) is the linear acceleration of the UAV in NED frame, estimated on the companion computer from differentiating the velocities.
\item \(vx_{m_{NED}},vy_{m_{NED}}\) is the linear velocity of the moving platform in NED frame, estimated from the velocity estimator for the moving platform,
\item \(\sigma^{2}_{IMU}\) is the variance of the velocities based estimated by the PX4's EKF.
\end{itemize}

In the correction step the observed measurements from the downward
looking camera will be used to correct and update the predicted
\emph{X,Y} position of the UAV. However due to the computation time
needed to detect the landing marker, that incoming measurement will be
delayed by some time \(dt_{obs}\). During that time \(dt_{obs}\) the
UAV will be relocated by an interval (\(dx_{obs},dy_{obs}\)). To
compensate that extra displacement, the interval
(\(dx_{obs},dy_{obs}\)) is calculated and added to the observed
measurements.

Correction step:
\begin{equation}
    \begin{array}{l}
    
    e_{t} = z_{t} - H_{t}*\hat{x}_{t} \\
    % [Ad: eq] I am not entirely familiar with this notation for EKF but I guess it should be P_t prior to the estimate so I would conveniently call it P_t^- so it does not confuse with P_t after the estimate (last line)
    S_{t} = H_{t}*P_{t}*H_{t}^\top + R_{t} \\
    K_{t} = P_{t}*H_{t}^\top * S_{t}^{-1} \\
    \hat{x}_{t} = \hat{x}_{t} + K_{t} * e_{t} \\
    P_{t} = (I - K_{t}*H_{t}) * P_{t}
    \end{array}
\end{equation}

where:
\begin{equation*}
    \begin{array}{l}
    z_{t} = \begin{bmatrix} 
                x_{obs} + dx_{obs} \\ 
                y_{obs} + dy_{obs}
                \end{bmatrix}, \quad 


    
    H_{t}= \begin{bmatrix} 
                1 & 0 \\ 
                0 & 1
                \end{bmatrix}, \quad
    R_{t} = \begin{bmatrix} 
                1 & 0 \\ 
                0 & 1
                \end{bmatrix}*\sigma^{2}_{obs}  \\ \\
                
dx_{obs} = vx_{mean_{obs}} * dt_{obs} \quad , \quad
    dy_{obs} = vy_{mean_{obs}} * dt_{obs} \quad \\
          
    \end{array}
\end{equation*}

A velocity estimator is also constructed to determine the magnitude
and direction of the moving platform's velocity vector. The magnitude
is calculated by differentiating two sequential \emph{X,Y} position
measurements of the moving platform and taking into consideration the
UAV's \emph{NED} velocities according to the PX4's EKF. A low-pass
filter is used to provide a smooth estimation of the velocity's
magnitude by filtering out high frequency noise. High frequency noise
can be caused by oscillations of the UAV along with a fast update rate
in the landing marker detection algorithm.  \hemi{Please explain why
  the signal is the \emph{smoothest}.}

In the agricultural use-case the moving platform is likely to change
its direction up to 180 degrees. Furthermore, it is assumed that the
moving platform is a nonholonomic system, like a tractor. To
compensate for sudden turns, the moving platform's yaw orientation
will be taken into account and based on the velocity's magnitude, the
moving platform's velocity in \emph{X,Y} \emph{NED} frame can be obtained.


\subsection{Wind Disturbances}
\label{sec:WindDisturbances}

The exact and accurate estimation of the applied wind forces on a body
is a complex matter studied by the field of fluid dynamics. We use a
simplified approach, as follows. We assume that the wind will be
applied on an area of \SI{0.09}{\m^2}. Such an area is emulating a UAV
with extra payload attached on its frame. Two different wind speeds of
\SI{8}{\m \per \s} and \SI{12}{\m \per \s} will be used to calculate the applied wind forces
on that area. The wind forces are considered to be applied on the
center of gravity of the UAV with direction parallel to the ground.

The magnitude of the applied force, corresponding to a certain wind
speed is calculated by the following equations
\cite{Dynamic_pressure_NASA,anderson2010fundamentals}:

\begin{equation}
    \begin{array}{l}
         F = p_{d}  * A  \\
         p_{d} = \frac{\varrho * v^2}{2} 
    \end{array}
\end{equation}

where: $F$ is the force, \(p_{d}\) is the dynamic pressure, \(A\) is
the area of the applied pressure, \(\varrho\) is the density of the
air (around \SI{1.2}{\kg \per \m^3}), \(v\) is the wind velocity in \SI{}{\m \per \s}.

By solving the above equations the applied force on the UAV is found
to be \SI{3.45}{\newton} for an \SI{8}{\m \per \s} wind speed, and \SI{7.76}{\newton} for a \SI{12}{\m \per \s}
wind speed. The wind forces will be applied on the UAV in Gazebo
simulation, to test the performance of the whole system. This will be
done by constructing a program that will apply the wind forces on the
virtual UAV.

To simulate a wind pattern the wind direction and magnitude must be
defined. The wind direction is assumed to remain the same for the
whole duration of the experiments. That direction vector is defined as
\emph{[0.8, 0.2]} according to Gazebo's \emph{(x,y)} axes.
%
The magnitude of the wind is calculated as follows: %according to the following pattern:

\begin{itemize}
    \item An initial wind power of either \SI{3.45}{\newton} or \SI{7.76}{\newton} is chosen.
    
    \item An update cycle of \SI{5.5}{\sec} is chosen between two different applied forces.
    
    \item A random float number between 0.9 and 1.2 is chosen at every update cycle. This number is defined as \emph{Random\_noise}.
    
    \item The applied wind force is calculated as:\\
    
  
    
        \(\mbox{\em force}_{x} = -x_{norm} * \mbox{\em power}_{init} * \mbox{\em Random\_noise} \\
         \mbox{\em force}_{y} = -y_{norm} * \mbox{\em power}_{init} * \mbox{\em Random\_noise}\)\\
        

    where \(x_{norm } = 0.8\), \(y_{norm}  = 0.2\) , \(\mbox{\em power}_{init} = 7.0 N\)
    
    \item For a UAV altitude of \SI{6.0}{\meter} and below the power decreases as:
   \( \mbox{\em power} = \mbox{\em power}_{init}* (\mbox{\em altitude}/ 6.0)  \), where \(\mbox{\em power}_{init}\) is either \SI{3.45}{\newton} or \SI{7.76}{\newton}
   
   \item For a UAV altitude of \SI{3}{\meter} and below, \(\mbox{\em power} = 0.5\). 

    
\end{itemize}

This model is created to simulate different scaling in the wind's magnitude and will be used as it is for all simulated tests.

%%%%%%%%%%%%%%%%%%%%
\section{Evaluation}
\label{sec:experimental}

We evaluate our approach in terms of the quality of the overall
functionality and the energy efficiency of the algorithms. All
algorithms are executed on an embedded companion computer interfaced
to a simulation running on a standard computer.

\subsection{Use case: agricultural safety}

We evaluate our approach based on a simulated use case where %We demonstrate the use of 
a multirotor UAV identifies
hazardous objects around a moving platform, and lands on the
moving platform to recharge.
%
%The moving platform is following a path similar to a plowing tractor's and 
%
No communication link is considered between the moving platform and the UAV
and no GNSS positioning is assumed to be available. The system can thus be
considered as a fallback for fault-tolerance.

%For the first use, the UAV will track and follow the moving platform from a desired altitude, while detecting and classifying objects on the ground.

Object detection and classification is performed by feeding the input
image from the downward facing camera into a \emph{YOLOv3-tiny} CNN
\cite{yolov3} implemented in ROS \cite{bjelonicYolo2018}. Four
different classes are selected: cars, humans, tractors, and
cows. Based on the CNN's predictions and onboard
sensors, the UAV maps the detected objects onto a 2D map.
 
%For the second use, the UAV will track and land on the moving platform. 

A simulated field is created in Gazebo with objects placed in random
positions and orientations as seen in Figure~\ref{fig:Gazebo}.

\begin{figure}[h]
\centering
\includegraphics[scale=0.2]{gazebo_scalled.png}
\caption{On the left, a top view of the Gazebo scene. On the right, a view of the UAV attempting a landing on the moving platform }
\label{fig:Gazebo}
\end{figure}

Pre-trained weights for \emph{YOLOv3-tiny}, were initially used but the
performance on detecting objects from a downward facing camera was
not satisfactory. Since a dataset for detecting the above four classes
from a top view was not available, we created an artificial dataset
based on the Gazebo models. The dataset consists of 1200 images, 300
for each class. We trained the \emph{YOLOv3-tiny} by its default training
parameters for 5000 epochs, for an input image size of 416$\times$416
pixels.

%For the following experiments, the input image to $YOLOV3$ $tiny$ will be resized to a 416$\times$416 pixel size. 

\subsection{Experimental setup}

All experiments are performed in Gazebo simulation under Ubuntu 18.04
and ROS Melodic on a i7-8550U 1.8GHz (4.0GHz Boost), 8GB DDR4
laptop. The \emph{PX4} Software In The Loop (SITL) Firmware v1.10.2 is used as the flight
controller and the \emph{IRIS} quadcopter is used as the UAV platform. A
vertically facing RGB camera is placed on the UAV providing a
640$\times$480 pixel image at 10~fps. An \emph{Nvidia Jetson Nano}
with Ubuntu 18.04 and ROS Melodic is used as the UAV's companion
computer. All the computer vision, guidance, and control algorithms 
execute on the \emph{Nvidia Jetson Nano}, similar to how they would
be deployed if the Nano was a companion computer in a physical drone.

Two groups of experiments are conducted. The first group evaluates the
energy consumption and QoS of the \emph{tracking} mode and the second group
evaluates the energy consumption and QoS of the \emph{landing} mode. For
both groups, the experiments start with the tractor moving at a
constant speed of \SI{0.3}{\meter\per\second}, according to a square path similar to that of a
plowing tractor, and the UAV taking off and hovering at an altitude of
\SI{25}{\meter}. After reaching the desired altitude, the UAV starts searching
for the landing marker in the image frame. Once the landing marker is
detected, the UAV commences its actions.

On \emph{tracking} mode, the UAV will follow the moving platform at a fixed
altitude and use its vertically facing camera to map the environment
while on \emph{landing} mode, the UAV will follow the moving platform and
gradually lower its altitude until it lands on it. Both \emph{tracking} and
\emph{landing} modes are tested under three different cases of no wind
disturbances, wind disturbances of \SI{8}{\meter \per \second} and wind disturbances of
\SI{12}{\meter \per \second} according to the wind model described in Section~\ref{sec:WindDisturbances}.


\subsection{Results}

The first group of experiments was conducted to test the \emph{tracking} mode and evaluate its 
energy efficiency and QoS. For the energy 
evaluation, eight tests were executed for different fps rates 
for \emph{YOLOv3-tiny} ROS node (4fps, 1fps, 0.5fps, 0.1fps) and 
the \emph{landing marker} detection ROS node (10fps, 0.5fps) as 
%
seen in Figure~\ref{fig:PowerDuringTracking}. For a 4fps update rate
for \emph{YOLOv3-tiny}, a power consumption of \SI{6.30}{\watt} is observed while
for a 1fps and 0.1fps update rates, the power consumption drops to
\SI{4,8}{\watt} and \SI{3.9}{\watt} accordingly. By reducing the update rate for
\emph{landing marker} detection, from 10fps to 0.5fps, a further power
saving of \SI{0.15}{\watt}- \SI{0.19}{\watt} is achieved.



For the QoS evaluation, twelve tests were executed for different 
fps rates for the \emph{YOLOv3-tiny} ROS node (4fps, 0.1fps) and for 
the \emph{landing marker} detection ROS node (10fps, 0.5fps) under 
three different cases of wind disturbances. 
%
The QoS is determined as the number of correctly detected objects. An
object is considered to be correctly detected if it is detected within
a distance of \SI{2}{\meter} from the its actual position and classified with
the correct class. The detection results can be seen in
Figure~\ref{fig:NCorrectObjectDetections}. The best results were
obtained for a high fps update rate in \emph{YOLOv3-tiny} and \emph{landing marker} detection, under no wind disturbances, since 28 out of the 32
objects were detected. Nevertheless, for the same high fps values but
with a wind speed of \SI{12}{\meter \per \second}, only 18 out of 32 objects were
correctly detected.

%than the fps update rates since for higher wind speed, less objects are correctly detected.


\begin{figure}[h]
\centering
\includegraphics{data_visualization/PowerDetection.pdf}
\caption{Power consumption during tracking mode.}
\label{fig:PowerDuringTracking}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics{data_visualization/QoSDetection.pdf}
\caption{Number of correctly detected objects
under different conditions. 
Grey color denotes a 0.1fps and black color denotes a 4fps 
update rate in \emph{YOLOv3-tiny} ROS node. 
Circles denote a 0.5fps and triangles denote a 10fps update 
rate in the \emph{landing marker} detection ROS node.}
\label{fig:NCorrectObjectDetections}
\end{figure}


The second group of experiments was conducted to test the \emph{landing} mode
and evaluate its energy efficiency and QoS.
For the energy evaluation, eight tests were executed for different 
fps rates for the \emph{landing marker} detection ROS node 
(10fps, 2fps, 1fps, 0.5fps) under two cases. 
In the first case, the kernel size remains fixed at 
22$\times$22 pixels and in the second case an adaptive selection 
kernel size algorithm is used. 
%
The landing time is also taken under consideration as seen in
Figure~\ref{fig:PowerDuringLanding}. The largest difference in the
power consumption is \SI{0.14}{\watt} and is observed between the 0.5fps and
the 10fps update rate for the \emph{landing marker} detection. However
the landing time is reduced by \SI{30}{\second} when using the 10fps update
rate. The adaptive kernel size in most cases outperforms the fixed
kernel size by \SI{0.11}{\watt}. Furthermore the adaptive kernel size can
compensate for marker occlusions which will increase the overall
robustness of the system.

For the QoS evaluation, twelve tests were executed for different 
fps rates for the \emph{landing marker} detection ROS node 
(10fps, 2fps, 1fps, 0.5fps) under three different cases of wind
disturbances. The QoS is determined as the  mean squared error (MSE) 
between the predicted position of the moving platform, 
from the navigation block, and the moving platform's actual position. 
Four different altitude bins were used as seen in Figure~\ref{fig:PositionErrorDuringLanding}. A large MSE of around \SI{3}{\square\meter} is observed for an altitude greater than \SI{20}{\meter} for a 0.5fps rate, while a MSE close to zero is observed for an altitude of less than \SI{5}{\meter} for an update rate of 10fps. Furthermore, wind disturbances don't seem to have an influence on the MSE. An overall larger MSE for the \emph{y} coordinates than the \emph{x} coordinates is probably caused by a sudden change of the moving platform's direction on the \emph{y} axis.

\begin{figure}[h]
\centering
\includegraphics{data_visualization/PowerLanding.pdf}
\caption{Power consumption during landing mode. 
The black circles denote a fixed kernel size while the grey circles denote an adaptive kernel size.}
\label{fig:PowerDuringLanding}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics{data_visualization/QoSLanding.pdf}
\caption{Position error during landing and how it depends 
on the marker detection rate at different altitudes and wind disturbances.
Circles denote errors in the x direction and triangles 
errors in the y direction.}
\label{fig:PositionErrorDuringLanding}
\end{figure}

\subsection{Discussion}

The experiments show that both tracking mode and landing mode are
supported by the system, in a simulated environment with a moving
platform and random wind conditions. Moreover, the performance of both
modes is sensitive to the QoS, with a high success rate of both modes
at high QoS levels, and significantly lower performance at lower QoS
levels.

The potential energy savings from having an energy-sensitive algorithm
that can adapt its QoS by changing the fps values for the
\emph{YOLOv3-tiny} and \emph{landing marker} ROS nodes should be seen
in relation to the total energy consumption of the UAV. As a concrete
example, consider a DJI Phantom 4 multirotor and a Sky-Watch Cumulus
fixed-wing (the fixed-wing would need to circle while tracking and 
would need VTOL capabilities to land, but we nevertheless include it 
for comparison). We estimate\footnote{From information on the respective
  product pages regarding battery capacity and maximal flight time.}
that the Phantom uses roughly \SI{140}{\watt} while cruising whereas the Cumulus
uses roughly \SI{40}{\watt} while cruising. The maximal saving gained from
changing the \emph{YOLOv3-tiny} rate is $6.30W-3.9W=2.4W$ whereas the
maximal saving gained from changing the \emph{landing marker} rate is
\SI{0.2}{\watt}. For the Cumulus, there is thus a 6.5\% potential energy
saving, whereas the potential energy saving only is 1.9\% for the
Phantom. For the Cumulus this saving is thus large enough to
significantly impact the flying time of the drone, with a total energy
saving of \SI{23.4}{\kilo \joule}. For the Cumulus, the potential saving from adapting
the \emph{landing marker} QoS is however only 0.5\%. 

For the tracking mode, changing the \emph{landing marker} rate
provided a minor saving of \SI{0.14}{\watt}, but at the cost of increased
landing time. Therefore, although the higher-QoS computer vision
algorithm is marginally more expensive by \SI{0.14}{\watt}, the UAV will
overall save energy because of a reduced flight time.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion and Future Work}
\label{sec:conclusion}

In this paper we presented a robust, energy-sensitive, vision-based
algorithm for autonomous tracking and landing in varying environmental
conditions by executing all the necessary algorithms on the \emph{Nvidia Jetson Nano} companion computer.

% Proposed text by George:
Our experiments show that the proposed computer vision algorithms for detecting the moving platform can be run at the highest QoS level with only a marginal
energy overhead, whereas adapting the QoS level of \emph{YOLOv3-tiny} CNN results in a considerable power saving for the system as a whole. This power saving is significant if the system was executing on a fixed-wing UAV, %
but only marginal if executing on a multirotor UAV.

%Original text by Ulrik:
%Our experience show that the tracking and landing
%algorithms can be run at the highest QoS level with only a marginal
%energy overhead, whereas adapting the QoS level of all on-board
%algorithms results in a significant power saving for the system as a
%whole if it was executing on a fixed-wing drone, but only a marginal
%power saving on a multirotor drone.

In terms of future work, we are interested in automatically adapting
the QoS level to the available battery, and in testing this approach
on a physical drone.

%% acknowledgement
%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgment}

This work is supported and partly funded by the European Union’s
Horizon2020 research and innovation program under grant agreement
No.~779882 (TeamPlay).

\bibliographystyle{IEEEtran}
\bibliography{\jobname} 
\vspace{1ex}

\end{document}
