
%%
%% Conference Paper for IRC'20, November 9-11, Taichung, Taiwan
%% ***
%%

\documentclass[conference]{IEEEtran}

\newcommand{\stt}[1]{{\small\tt #1}} %\small\tt too small here
\newcommand{\powprof}{\stt{powprofiler}}
\newcommand{\figpath}{./figures}
\let\labelindent\relax

\usepackage[inline]{enumitem}
\usepackage{booktabs}
\usepackage{flushend}
\usepackage{tikz}

%% citation packege
\usepackage{cite}

%% figures package
%\usepackage[pdftex]{graphicx}
%\graphicspath{{figures/}}
%\DeclareGraphicsExtensions{.pdf,.jpeg,.png}

%% math package
\usepackage[cmex10]{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}

%% pseudocode package
%\usepackage{algorithmic}

%% packages for alignment
%\usepackage{array}
%\usepackage{mdwmath}
%\usepackage{mdwtab}
%\usepackage{eqparbox}

%% packages for subfigures (eventually)
\usepackage[tight,footnotesize]{subfigure}
%S\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
%\usepackage[caption=false,font=footnotesize]{subfig}

%% package for urls
\usepackage{url}

%% correct bad hyphenation here
\hyphenation{analysis}

%% references (generates a bib file for bibtex)
\begin{filecontents}{\jobname.bib}
@article{salami2014uav,
  title={UAV flight experiments applied to the remote sensing of vegetated areas},
  author={Salam{\'\i}, Esther and Barrado, Cristina and Pastor, Enric},
  journal={Remote Sensing},
  volume={6},
  number={11},
  pages={11051--11081},
  year={2014},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@inproceedings{costa2012use,
  title={The use of unmanned aerial vehicles and wireless sensor network in agricultural applications},
  author={Costa, Fausto G and Ueyama, J{\'o} and Braun, Torsten and Pessin, Gustavo and Os{\'o}rio, Fernando S and Vargas, Patr{\'\i}cia A},
  booktitle={2012 IEEE International Geoscience and Remote Sensing Symposium},
  pages={5045--5048},
  year={2012},
  organization={IEEE}
}
@inproceedings{seewald2020mechanical,
  title={Mechanical and Computational Energy Estimation of a Fixed-Wing Drone}, 
  author={Seewald, Adam and Garcia de Marina, Hector and Midtiby, Henrik Skov and Schultz, Ulrik Pagh},
  booktitle={Proceedings of the 2020 Fourth IEEE International Conference on Robotic Computing (IRC)},
  pages={to appear},
  year={2020},
  organization={IEEE} 
}
@inproceedings{saripalli2002vision,
  title={Vision-based autonomous landing of an unmanned aerial vehicle},
  author={Saripalli, Srikanth and Montgomery, James F and Sukhatme, Gaurav S},
  booktitle={Proceedings 2002 IEEE International Conference on Robotics and Automation (Cat. No. 02CH37292)},
  volume={3},
  pages={2799--2804},
  year={2002},
  organization={IEEE}
}

@inproceedings{saripalli2003landing,
  title={Landing on a moving target using an autonomous helicopter},
  author={Saripalli, Srikanth and Sukhatme, Gaurav S},
  booktitle={Field and service robotics},
  pages={277--286},
  year={2003},
  organization={Springer}
}

@inproceedings{lee2012autonomous,
  title={Autonomous landing of a VTOL UAV on a moving platform using image-based visual servoing},
  author={Lee, Daewon and Ryan, Tyler and Kim, H Jin},
  booktitle={2012 IEEE international conference on robotics and automation},
  pages={971--976},
  year={2012},
  organization={IEEE}
}

@inproceedings{chen2016system,
  title={System integration of a vision-guided UAV for autonomous landing on moving platform},
  author={Chen, Xudong and Phang, Swee King and Shan, Mo and Chen, Ben M},
  booktitle={2016 12th IEEE International Conference on Control and Automation (ICCA)},
  pages={761--766},
  year={2016},
  organization={IEEE}
}
@inproceedings{falanga2017vision,
  title={Vision-based autonomous quadrotor landing on a moving platform},
  author={Falanga, Davide and Zanchettin, Alessio and Simovic, Alessandro and Delmerico, Jeffrey and Scaramuzza, Davide},
  booktitle={2017 IEEE International Symposium on Safety, Security and Rescue Robotics (SSRR)},
  pages={200--207},
  year={2017},
  organization={IEEE}
}	
@article{araar2017vision,
  title={Vision based autonomous landing of multirotor UAV on moving platform},
  author={Araar, Oualid and Aouf, Nabil and Vitanov, Ivan},
  journal={Journal of Intelligent \& Robotic Systems},
  volume={85},
  number={2},
  pages={369--384},
  year={2017},
  publisher={Springer}
}
@article{feng2018autonomous,
  title={Autonomous landing of a UAV on a moving platform using model predictive control},
  author={Feng, Yi and Zhang, Cong and Baek, Stanley and Rawashdeh, Samir and Mohammadi, Alireza},
  journal={Drones},
  volume={2},
  number={4},
  pages={34},
  year={2018},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@article{kyristsis2016towards,
  title={Towards autonomous modular UAV missions: The detection, geo-location and landing paradigm},
  author={Kyristsis, Sarantis and Antonopoulos, Angelos and Chanialakis, Theofilos and Stefanakis, Emmanouel and Linardos, Christos and Tripolitsiotis, Achilles and Partsinevelos, Panagiotis},
  journal={Sensors},
  volume={16},
  number={11},
  pages={1844},
  year={2016},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@inproceedings{acuna2018vision,
  title={Vision-based UAV landing on a moving platform in GPS denied environments using motion prediction},
  author={Acuna, Raul and Zhang, Ding and Willert, Volker},
  booktitle={2018 Latin American Robotic Symposium, 2018 Brazilian Symposium on Robotics (SBR) and 2018 Workshop on Robotics in Education (WRE)},
  pages={515--521},
  year={2018},
  organization={IEEE}
}
@inproceedings{lee2016vision,
  title={Vision-based UAV landing on the moving vehicle},
  author={Lee, Hanseob and Jung, Seokwoo and Shim, David Hyunchul},
  booktitle={2016 International conference on unmanned aircraft systems (ICUAS)},
  pages={1--7},
  year={2016},
  organization={IEEE}
}
@article{nguyen2018lightdenseyolo,
  title={LightDenseYOLO: A fast and accurate marker tracker for autonomous UAV landing by visible light camera sensor on drone},
  author={Nguyen, Phong Ha and Arsalan, Muhammad and Koo, Ja Hyung and Naqvi, Rizwan Ali and Truong, Noi Quang and Park, Kang Ryoung},
  journal={Sensors},
  volume={18},
  number={6},
  pages={1703},
  year={2018},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@article{guo2020precision,
  title={Precision Landing Test and Simulation of the Agricultural UAV on Apron},
  author={Guo, Yangyang and Guo, Jiaqian and Liu, Chang and Xiong, Hongting and Chai, Lilong and He, Dongjian},
  journal={Sensors},
  volume={20},
  number={12},
  pages={3369},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@article{yuan2018hierarchical,
  title={A hierarchical vision-based localization of rotor unmanned aerial vehicles for autonomous landing},
  author={Yuan, Haiwen and Xiao, Changshi and Xiu, Supu and Zhan, Wenqiang and Ye, Zhenyi and Zhang, Fan and Zhou, Chunhui and Wen, Yuanqiao and Li, Qiliang},
  journal={International Journal of Distributed Sensor Networks},
  volume={14},
  number={9},
  year={2018}
  publisher={SAGE Publications Sage UK: London, England}
}
@article{yang2018hybrid,
  title={Hybrid camera array-based uav auto-landing on moving ugv in gps-denied environment},
  author={Yang, Tao and Ren, Qiang and Zhang, Fangbing and Xie, Bolin and Ren, Hailei and Li, Jing and Zhang, Yanning},
  journal={Remote Sensing},
  volume={10},
  number={11},
  pages={1829},
  year={2018},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@inproceedings{olson2011apriltag,
  title={AprilTag: A robust and flexible visual fiducial system},
  author={Olson, Edwin},
  booktitle={2011 IEEE International Conference on Robotics and Automation},
  pages={3400--3407},
  year={2011},
  organization={IEEE}
}
@article{berenz2012autonomous,
  title={Autonomous battery management for mobile robots based on risk and gain assessment},
  author={Berenz, Vincent and Tanaka, Fumihide and Suzuki, Kenji},
  journal={Artificial Intelligence Review},
  volume={37},
  number={3},
  pages={217--237},
  year={2012},
  publisher={Springer}
}
@inproceedings{mei2005case,
  title={A case study of mobile robot's energy consumption and conservation techniques},
  author={Mei, Yongguo and Lu, Yung-Hsiang and Hu, Y Charlie and Lee, CS George},
  booktitle={ICAR'05. Proceedings., 12th International Conference on Advanced Robotics, 2005.},
  pages={492--497},
  year={2005},
  organization={IEEE}
}
@inproceedings{mei2004energy,
  title={Energy-efficient motion planning for mobile robots},
  author={Mei, Yongguo and Lu, Yung-Hsiang and Hu, Y Charlie and Lee, CS George},
  booktitle={IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA'04. 2004},
  volume={5},
  pages={4344--4349},
  year={2004},
  organization={IEEE}
}
@article{mei2006deployment,
  title={Deployment of mobile robots with energy and timing constraints},
  author={Mei, Yongguo and Lu, Yung-Hsiang and Hu, Yu Charlie and Lee, CS George},
  journal={IEEE Transactions on robotics},
  volume={22},
  number={3},
  pages={507--522},
  year={2006},
  publisher={IEEE}
}
@article{morales2009power,
  title={Power consumption modeling of skid-steer tracked mobile robots on rigid terrain},
  author={Morales, Jesus and Martinez, Jorge L and Mandow, Anthony and Garc{\'\i}a-Cerezo, Alfonso J and Pedraza, Salvador},
  journal={IEEE Transactions on Robotics},
  volume={25},
  number={5},
  pages={1098--1108},
  year={2009},
  publisher={IEEE}
}
@article{seewald2019coarse,
  title={Coarse-Grained Computation-Oriented Energy Modeling for Heterogeneous Parallel Embedded Systems},
  author={Seewald, Adam and Schultz, Ulrik Pagh and Ebeid, Emad and Midtiby, Henrik Skov},
  journal={International Journal of Parallel Programming},
  pages={1--22},
  year={2019},
  publisher={Springer}
}
@inproceedings{nardi2015introducing,
  title={Introducing SLAMBench, a performance and accuracy benchmarking methodology for SLAM},
  author={Nardi, Luigi and Bodin, Bruno and Zia, M Zeeshan and Mawer, John and Nisbet, Andy and Kelly, Paul HJ and Davison, Andrew J and Luj{\'a}n, Mikel and O'Boyle, Michael FP and Riley, Graham and others},
  booktitle={2015 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={5783--5790},
  year={2015},
  organization={IEEE}
}
@article{sadrpour2013mission,
  title={Mission Energy Prediction for Unmanned Ground Vehicles Using Real-time Measurements and Prior Knowledge},
  author={Sadrpour, Amir and Jin, Jionghua and Ulsoy, A Galip},
  journal={Journal of Field Robotics},
  volume={30},
  number={3},
  pages={399--414},
  year={2013},
  publisher={Wiley Online Library}
}
@inproceedings{sadrpour2013experimental,
  title={Experimental validation of mission energy prediction model for unmanned ground vehicles},
  author={Sadrpour, Amir and Jin, Judy and Ulsoy, A Galip},
  booktitle={2013 American Control Conference},
  pages={5960--5965},
  year={2013},
  organization={IEEE}
}
\end{filecontents}

\begin{document}

%% [Ad: typo] ...quadrocopter... -> ...quadcopter... {I would actually use multirotor UAV / not only quadrotors}
%% [Ad: little suggestion] An Energy-Efficient Vision-Based Autonomous Tracking and Landing Approach for a Multirotor UAV on a Moving Platform in an Agricultural Use-Case


% [Ad: Quadrocopter; so it should work as well despite what I said earlier, but I checked IEEExplore and Quadrotor is far more used]
\title{Energy-Sensitive Vision-Based Autonomous Tracking and Landing of a Quadrocopter on a Moving Platform} %for an Agricultural Application %UPS: suggested title, agriculture less important, playing with "energy sensitive" since we know about the energy and can adapt QoS accordingly

%% author names and affiliations
\author{
\IEEEauthorblockN{Georgios Zamanakos, Adam Seewald, Henrik Skov Midtiby, and Ulrik Pagh Schultz}
\IEEEauthorblockA{SDU UAS Center, M{\ae}rsk Mc-Kinney M{\o}ller Institute\\
University of Southern Denmark\\
Email: \{*\}@mmmi.sdu.dk
}}

%% make the title area
\maketitle


\begin{abstract}

%\boldmath
abstract\\
abstract\\
abstract\\
abstract\\
abstract\\
abstract\\
abstract\\
abstract\\
abstract\\
abstract\\
abstract\\
abstract\\
abstract\\
abstract

\end{abstract}

% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:introduction}

Unmanned Aerial Vehicles (UAVs) are increasingly used for applications such as monitoring, surveillance, transportation of small payloads, and agricultural applications~\cite{costa2012use,salami2014uav}. 
%
One of the major constraints of such applications is their limited level of autonomy due to battery limitations. %It is therefore seen that to further 
Extending the flying time of a UAV is normally done by having it land in order to replace or charge the battery before continuing the mission. Performing landings autonomously can however be challenging depending on the environment and whether the landing platform is stationary or mobile.
%
%Apart from presenting an increased energy efficiency concerning the computations, our approach further elaborates on the landing of the UAV on a moving platform. It can be easily shown that it is not generally energy- or time-efficient for the UAV to fly back to its base, once the battery reaches a critical level. Instead it is proposed that the UAV should land on the moving vehicle that is inherently in the proximity of the UAV during normal operation. 
%
Moreover, relying solely on the availability of a GPS signal for autonomous precision landing is not considered safe, since GPS signals can be temporarily lost or even tampered with. %The noise and errors of the GPS signal are not predictable and instead a 
As an alternative, in this paper we investigate the use of a novel vision-based autonomous landing system, and evaluate its robustness towards environmental conditions such as visual disturbances and wind. %is considered a more optimal option.

Extension of the flight time can be also achieved by using \emph{energy-sensitive algorithms} that can reduce energy consumption by reducing the quality of service (QoS). With this approach, energy-costly computations such as computer vision are adapted by selecting the desired quality of service (QoS) to match the available energy~\cite{seewald2020mechanical}. %With such an approach, 
By combining energy-sensitive algorithms with autonomous landing capabilities, we aim to increase the total availability of the UAV to perform operations, by extending the flight time and using autonomous recharging when needed.
%reduce the energy consumption of the UAV during flight and furthermore increase the level of autonomy.

The main contribution of this paper concerns the experimental study of a robust, energy-sensitive, vision-based algorithm for autonomous landing in varying environmental conditions. The algorithms are executed on an NVIDIA Jetson Nano companion computer controlling a simulated drone using a hardware-in-the-loop experimental setup. The vision-based landing algorithms provide novel capabilities in terms of tolerance to visual disturbance and varying environmental conditions such as wind.
%
Our experiments are based on an agricultural use case where a quadrocopter UAV performs visual identification of ground-based hazards while tracking and landing on a moving tractor. %will be studied.

%%%%%%%%%%%%%%%%%%%%%%
\section{State of the Art}
\label{sec:state-of-the-art}

Vision-based autonomous landing on a marker has been extensively studied by many researchers. Key distinctions include whether the marker is on a moving platform, the type of the marker, the algorithms used to detect it, as well as the mounted sensors on-board of the UAV.

For stationary platforms, one of the first experiments with vision-based autonomous landing was conducted by Saripalli et~al.~\cite{saripalli2002vision}. Here, a helicopter with a color camera facing vertically towards the ground would land on an ”H”-shape pattern (similar to ones found on a helipad) using a hierarchical behavior-based control architecture. In physical tests a marker of 122cm$\times$122cm size was detected for a maximum altitude of 10m. A landing marker inspired by a QR code but consisting of three artificial markers is demonstrated by Yuan et~al.~\cite{yuan2018hierarchical}, and was shown to provide a 6-DOF pose over an altitude range of 0-20m. Our work is however focused on the ability to land on moving platforms.

Saripalli et~al.~\cite{saripalli2003landing} also demonstrated the use of a Kalman Filter to track a moving platform. However all the computations were performed offline. Similarly, an ArUco marker was used as a landing marker by Lee et~al.~\cite{lee2012autonomous} to detect a moving platform. The control of the UAV is performed based on the error provided by the vision algorithm but all the computations were performed off-board. Arrar, et~al.~\cite{araar2017vision} focus on extending the detection range by using an AprilTag~\cite{olson2011apriltag} as a landing marker. Again all the computer vision algorithms were also executed off-board. A crucial aspect of our application is to perform all the computations on-board, and to evaluate them according to their energy efficiency as a function of QoS.

The design of the marker and choice of sensors can facilitate doing the computations on-board. %Is this the point of this paragraph?
%
Chen et~al.~\cite{chen2016system} utilized a marker consisting of a circle and rectangles of different colors along with a LiDAR scanning range finder for height estimation. The marker was detected by performing color segmentation on the incoming image frame. By fusing the height measurement from the LiDAR into the vision measurement, a relative pose of the UAV from the moving platform was obtained. A color segmentation approach was also implemented by Lee et~al.~\cite{lee2016vision}. A red rectangle was used as a landing marker and a vertically facing camera with a fish-eye lens was used to detect it, and a successful landing from an altitude of 70m was demonstrated. Both teams have used an on-board companion computer to perform all the computation on the UAV. However a color segmentation approach is not considered as a safe option for a realistic (outdoor) case it would be difficult, if not impossible, to ensure that the landing marker will be the only object of a specific color in the scene.

The use of a hybrid camera system consisting of a fish-eye IR camera and a stereo camera was demonstrated by Yang et~al.~\cite{yang2018hybrid}. An ArUco marker was used to mark the moving platform and a convolutional neural network (CNN) Yolo~v3 was trained specifically for marker detection. A similar approach concerning the detection of a landing marker was demonstrated by Nguyen et~al.~\cite{nguyen2018lightdenseyolo}. Here a specific landing marker was used and a specific CNN was used to detect it: successful detection of a 1m$\times$1m marker size was demonstrated from a distance of 50m. An AprilTag marker was used as a landing marker by Kyritsis et~al.~\cite{kyristsis2016towards} for the purpose of ``2016 DJI Developer Challenge''. The identification of the AprilTag marker was performed through Graphics Processing Unit (GPU). The three teams have utilized the companion's computer GPU to detect the landing marker. In the agricultural use case addressed in this paper, the GPU is however needed for a CNN to detect ground hazards, and since the GPU cannot simultaneously run different algorithms, %objects around the moving platform and 
the CPU should be used for detecting the landing marker. By doing so, a different QoS could be chosen for each algorithm.    

% [Ad: energy-sensitive part {begin}]
To account for the energy modeling of computer vision algorithms, we considered the work previously carried by Nardi et~al.~\cite{nardi2015introducing}. The authors present SLAMBench, a framework that investigates SLAM algorithms configuration alternatives for energy efficiency. Whereas, we use \powprof{}, a generic energy modeling tool. The tool, presented at an earlier point in our work~\cite{seewald2019coarse}, accounts for measuring the energy impact of different configurations of the ROS based system featuring the agricultural use-case.

Some further energy modeling, such as mission-based energy models studied by Sadrpour et~al.~\cite{sadrpour2013experimental, sadrpour2013mission}, focus mostly on ground-based autonomous vehicles instead of the UAVs. Others investigated extensively the concept of motion, as the work presented by Morales et al.~\cite{morales2009power}, but do not account further on the computation.

Energy modeling of mobile robots specifically, as the work carried by Mei et al.~\cite{mei2006deployment, mei2005case, mei2004energy}, has provided the ground for the concept of modeling computation for energy-sensitive algorithm design. The authors' approach has evolved from an energy-efficient motion planning technique in~\cite{mei2004energy}, a design strategy that allows accounting for motion and computations separately in~\cite{mei2005case}, to an energy-efficient deployment algorithm in~\cite{mei2006deployment}.

The battery in our system is considered in the context of a drone being able to perform its mission while accounting for the eventuality of a battery shortage; to this end, we investigated the approach presented by Berenz et~al.~\cite{berenz2012autonomous}, where a battery management mission-based dynamic recharge approach is presented. A set of recharge stations are used, along with self-docking capable robots. Our approach similarly allows landing on the platform, which is inherently in the proximity of the drone.
% [Ad: energy-sensitive part {end} ]

Taking into account varying environmental conditions and unpredictable movements of the platform to land on is relevant for the use of landing in outdoor, mobile scenarios.
Regarading wind conditions, an AprilTag marker was used by Feng et~al.~\cite{feng2018autonomous} with a constant wind speed of 5~m/s as an external disturbance in a simulation environment. Nevertheless, a fluctuation in the wind's magnitude and direction is likely to happen in realistic cases.
%/
Concerning estimation of the moving platform's position and velocity, a Kalman Filter or Extended Kalman Filter has been used for the estimation~\cite{araar2017vision,feng2018autonomous, falanga2017vision}, whereas Yang et~al.~\cite{yang2018hybrid} constructed a velocity observer algorithm by calculating the actual moving distance of the moving platform over a period of time. 















%One of the first attempts for a vision-based autonomous landing was conducted by Saripalli et al. \cite{saripalli2002vision} in which a helicopter, with a color camera facing vertically towards the ground, was used as an aerial platform. An "H" shape pattern similar to ones found on a helipad was chosen to mark the landing site and a hierarchical behavior-based control architecture was used to control the helicopter. According to real tests, an "H" landing marker of 122cm X 122cm size, was effectively detected for a maximum altitude of 10m. Saripalli et al. \cite{saripalli2003landing} extended their prior work and used a Kalman Filter to track a moving landing site. Real flights were conducted in both cases.

%An ArUco marker was used by Lee et al. \cite{lee2012autonomous} to detect a moving platform. The control of the UAV is performed based on the error provided by the vision algorithm but all the computations are performed off-board. A hierarchy of behaviors is also implemented by this team. The overall performance of the system is considered satisfactory according to the experimental results, however a detection of the marker at an altitude of around 1m was demonstrated.

%A contribution by Chen at al. \cite{chen2016system} utilizes a marker consisting of a circle and rectangles of different colors along with a LiDAR scanning range finder for height estimation. Both sensors are installed on a self-customized quadcopter UAV. The marker is detected by performing color segmentation on the incoming image frame. By fusing the height measurement from the LiDAR into the vision measurement, a relative pose of the UAV from the moving platform is obtained. Test flights were conducted indoors, at a maximum altitude of 3m, and successful landings were performed on a moving platform with speed of 1 m/s. A limitation of this system is that the quadcopter is not able to localize itself if the marker is not in the view of the on-board vertical camera.

%A red rectangle is used as a landing marker by Lee et al \cite{lee2016vision}. The team has placed on a quadcopter UAV a vertically faced camera with a fish-eye lense. The landing marker is detected by performing color segmentation. The relative distance of the UAV from the moving target is used by the target tracking algorithm to generate the acceleration command for the UAV. Real tests are performed both on static and moving landing platforms at a maximum altitude of 70 meters.

%A hybrid camera system consisting of a fish-eye IR camera and a stereo camera is demonstrated by Yang et al. \cite{yang2018hybrid}. The fish-eye IR camera is used to obtain a wider field of view and the stereo camera to obtain depth information. An ArUco marker is used to mark the moving platform and the convolutional neural network (CNN) Yolov3 is trained to detect the ArUco marker. The team has implemented different filtering algorithms to remove false positive cases concerning the detection of the marker. Simulation and real tests are conducted from an altitude of 8m and 4m respectively.



 
%In 2017, Arrar, et al. \cite{araar2017vision} focused on extending the detection range by using an AprilTag  \cite{olson2011apriltag} as a landing marker. The vision-based measurements were fused with the inertial measurements through an Extended Kalman Filter (EKF). Real tests flights, at an altitude of 80cm, were conducted indoors to demonstrate the efficiency of the algorithms. 



%An AprilTag marker is used as a landing marker by Kyritsis et al. \cite{kyristsis2016towards} for the purpose of "2016 DJI Developer Challenge". The identification of the AprilTag marker is performed through Graphics Processing Unit (GPU) parallelized processing. The team has studied the scenario of a UAV performing a search mission and then landing on a moving target. Real tests with a DJI Matrice 100 UAV platform were conducted. Successful landings are achieved up to a speed of 30 km/h for the moving target, in this case a car.

%An AprilTag marker was also used by Feng et al. \cite{feng2018autonomous}. The team has used a Kalman Filter to estimate the pose and speed of the moving platform and a model predictive control approach for controlling the UAV. A constant wind speed of 5 m/s is used as an external disturbance in simulation environment. A straight and curved path is used for the moving platform. The team has conducted simulation flights and demonstrated the efficiency of their system under noisy measurements and wind disturbances.

%A different approach concerning the detection of a landing marker was demonstrated by Nguyen et al \cite{nguyen2018lightdenseyolo}. The team trained a CNN named lightDenseYOLO to detect a marker's location. An effective detection of a marker size 1m X 1m was demonstrated up to a distance of 50m. Furthermore the team has managed to confirm their method under various environmental conditions and distances but failed to detect the marker under high occlusions and when the target was moving too fast.


%A landing marker inspired by a QR code is demonstrated by Yuan et al. \cite{yuan2018hierarchical}. The landing marker consists of three artificial markers, denoted as Top,Right and Bottom. Each one of the artificial markers consists of white and black squares. The team has demonstrated that the landing marker can provide a 6-DOF pose over an altitude range of 0-20m.



%Concerning agricultural use cases, Guo et al \cite{guo2020precision} has studied the use of UAVs in an agricultural application and especially for landing inside a poultry house. A number of fiducial markers of different sizes is used to mark the landing site and tests were conducted in simulation environment.


%Studying the state of the art of vision-based autonomous landing it is concluded that all the previous works lack at least one of the following properties:

%\begin{enumerate}

%\item An evaluation of the system's performance under dynamic wind disturbances.

%\item An evaluation of the system's performance for different update rates and QoS, concerning the computer vision algorithms for detecting the landing marker.

%\item An evaluation of the system's performance for an agricultural outdoor case in which the moving platform is moving irregularly with sudden turns of 180 degrees.

%\end{enumerate}

%Furthermore a new landing marker pattern is proposed that is highly robust to occlusions and can be detected effectively from large distances. 

%To perform this evaluation, a complete structure of algorithms concerning the detection of the landing marker, the estimation of the UAV's relative pose in world coordinates, the estimation of the moving platform's velocity, the path planning, the trajectory generation and the control algorithms will also be presented in this paper.

%In session II the materials will be presented, in session III ... ( and so on) 

%%%%%%%%%%%%%%%%%%%%%%%%%%%
% [Ad: change in the title of the section] I would actually change the title to something like Energy-Sensitive Mission Deployment, as in the current shape the title does not account for hazard detection (yolo) part
\section{Energy-Sensitive Control of Tracking and Landing}
\label{sec:approach}

% Idea: fairly short section painting the overall picture of our concept
% for making the UAV control be energy sensive. First describe energy
% sensitivity as a concept, based on powprofiler (QoS->energy
% prediction) and the other IEEE RC paper as an example. Then describe
% the overall tracking and lan/ding algorithm abstractly (pseudo code
% outline of what it does) and what the relevant QoS levels are (i.e.,
% rate at which the marker detector runs). Include also a ``black-box''
% workload representing whatever other work is being done as whatever
% QoS, this will be the CNN.

% [Ad: added a little description of energy-sensitive design]
The energy-sensitive design is a mission-oriented concept that adjusts the computations to the mission being performed while taking into account energy requirements, such as the presence of a limited power source. Specifically, in the agricultural use-case, the concept is employed to level the computationally heavy algorithms:  autonomous tracking, landing, and hazard detection, and is achieved in simulation iteratively in three steps. 

First, the developer specifies the maximum and minimum QoS level per each algorithm the use-case is composed of. The levels are statically defined, an automatic generation during different phases of the mission is being currently investigated and is considered future work. The algorithms are wrapped using ROS middleware, requiring the developer to specify the current ROS configuration through a configuration file in a key-value pair format which is then interpreted by \powprof{}.

Second, \powprof{} evaluates the energy consumption empirically evaluating a number of possible combinations and inferring the others by the means of a multivariate linear interpolation. The First two steps are iterated in the simulated environment with different configurations. 
% [Ad: Georgios, could you check those make sense? I am just sure for the hazard detection, not the others]
Namely, the autonomous tracking allows changing the QoS tracking step in ms, landing the QoS landing step also in ms, and hazard detection the QoS frames-per-second (FPS) rate the CNN runs at.

In the final step, the model is assessed and the QoS is adjusted to the desired granularity of the algorithms, along with the energy requirements.

%%%%%%%%%%%%%%%%%%%%
\section{Vision-Based Autonomous Tracking and Landing}
\label{sec:landing}

TODO: Georgios: your key contribution here

%%%%%%%%%%%%%%%%%%%%
\section{Evaluation}
\label{sec:experimental}

\subsection{Use case: agricultural safety}

Briefly describe the use case and simulation, including the use of CNN
to detect

\subsection{Experimental setup}

How the experiment will be done concretely, i.e., time to land as a
function of QoS and wind or whatever.

\subsection{Results}

Results of the experiments

\subsection{Discussion}

Discussion of the results

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion and Future Work}
\label{sec:conclusion}

What we did, why it was exciting, and what we want to do.

%% acknowledgement
%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgment}

This work is supported and partly funded by the European Union’s Horizon2020 research and innovation program under grant agreement No.~779882 (TeamPlay).

\bibliographystyle{IEEEtran}
\bibliography{\jobname} 
\vspace{1ex}

\end{document}
