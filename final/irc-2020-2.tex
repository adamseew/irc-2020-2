
%%
%% Conference Paper for IRC'20, November 9-11, Taichung, Taiwan
%% ***
%%

\documentclass[conference]{IEEEtran}
%\documentclass[conference, onecolumn, draftclsnofoot]{IEEEtran}

\newcommand{\stt}[1]{{\small\tt #1}} %\small\tt too small here
\newcommand{\powprof}{\stt{powprofiler}}
\newcommand{\figpath}{./figures}
\let\labelindent\relax

%\usepackage{lineno}
%\linenumbers
%\usepackage[left=4cm, right=7cm]{geometry}
%\setlength{\marginparwidth}{4cm}
\usepackage[inline]{enumitem}
\usepackage{booktabs}
\usepackage{flushend}
\usepackage{tikz}
\usepackage{siunitx}
\sisetup{per-mode = symbol}
\DeclareSIUnit\fps{fps}
\usepackage{todonotes}
\setuptodonotes{inline}
%\newcommand{\adam}[2][]{\todo[color=orange!20, #1]{ADAM: #2}}
%\newcommand{\hemi}[2][]{\todo[color=green!20, #1]{HEMI: #2}}


%% citation packege
\usepackage{cite}

%% figures package
\usepackage{graphicx}
\graphicspath{{figures/}}
%\DeclareGraphicsExtensions{.pdf,.jpeg,.png}

%% math package
\usepackage[cmex10]{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}

%% pseudocode package
%\usepackage{algorithmic}

%% packages for alignment
%\usepackage{array}
%\usepackage{mdwmath}
%\usepackage{mdwtab}
%\usepackage{eqparbox}

%% packages for subfigures (eventually)
\usepackage[tight,footnotesize]{subfigure}
%S\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
%\usepackage[caption=false,font=footnotesize]{subfig}

%% package for urls
\usepackage{url}

%% correct bad hyphenation here
\hyphenation{analysis}

%% references (generates a bib file for bibtex)
\begin{filecontents}{\jobname.bib}
@article{salami2014uav,
  title={{UAV} flight experiments applied to the remote sensing of vegetated areas},
  author={Salam{\'\i}, Esther and Barrado, Cristina and Pastor, Enric},
  journal={Remote Sensing},
  volume={6},
  number={11},
  pages={11051--11081},
  year={2014},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@inproceedings{costa2012use,
  title={The use of unmanned aerial vehicles and wireless sensor network in agricultural applications},
  author={Costa, Fausto G and Ueyama, J{\'o} and Braun, Torsten and Pessin, Gustavo and Os{\'o}rio, Fernando S and Vargas, Patr{\'\i}cia A},
  booktitle={2012 IEEE International Geoscience and Remote Sensing Symposium},
  pages={5045--5048},
  year={2012},
  organization={IEEE}
}
@inproceedings{seewald2020mechanical,
  title={Mechanical and Computational Energy Estimation of a Fixed-Wing Drone}, 
  author={Seewald, Adam and Garcia de Marina, Hector and Midtiby, Henrik Skov and Schultz, Ulrik Pagh},
  booktitle={2020 4th IEEE International Conference on Robotic Computing (IRC)},
  pages={to appear},
  year={2020},
  organization={IEEE} 
}
@inproceedings{saripalli2002vision,
  title={Vision-based autonomous landing of an unmanned aerial vehicle},
  author={Saripalli, Srikanth and Montgomery, James F and Sukhatme, Gaurav S},
  booktitle={2002 IEEE International Conference on Robotics and Automation (ICRA)},
  volume={3},
  pages={2799--2804},
  year={2002},
  organization={IEEE}
}

@inproceedings{saripalli2003landing,
  title={Landing on a moving target using an autonomous helicopter},
  author={Saripalli, Srikanth and Sukhatme, Gaurav S},
  booktitle={Field and Service Robotics},
  pages={277--286},
  year={2003},
  organization={Springer}
}

@inproceedings{lee2012autonomous,
  title={Autonomous landing of a {VTOL UAV} on a moving platform using image-based visual servoing},
  author={Lee, Daewon and Ryan, Tyler and Kim, H Jin},
  booktitle={2012 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={971--976},
  year={2012},
  organization={IEEE}
}

@inproceedings{chen2016system,
  title={System integration of a vision-guided {UAV} for autonomous landing on moving platform},
  author={Chen, Xudong and Phang, Swee King and Shan, Mo and Chen, Ben M},
  booktitle={2016 12th IEEE International Conference on Control and Automation (ICCA)},
  pages={761--766},
  year={2016},
  organization={IEEE}
}
@inproceedings{falanga2017vision,
  title={Vision-based autonomous quadrotor landing on a moving platform},
  author={Falanga, Davide and Zanchettin, Alessio and Simovic, Alessandro and Delmerico, Jeffrey and Scaramuzza, Davide},
  booktitle={2017 IEEE International Symposium on Safety, Security and Rescue Robotics (SSRR)},
  pages={200--207},
  year={2017},
  organization={IEEE}
}	
@article{araar2017vision,
  title={Vision based autonomous landing of multirotor {UAV} on moving platform},
  author={Araar, Oualid and Aouf, Nabil and Vitanov, Ivan},
  journal={Journal of Intelligent \& Robotic Systems},
  volume={85},
  number={2},
  pages={369--384},
  year={2017},
  publisher={Springer}
}
@article{feng2018autonomous,
  title={Autonomous landing of a {UAV} on a moving platform using model predictive control},
  author={Feng, Yi and Zhang, Cong and Baek, Stanley and Rawashdeh, Samir and Mohammadi, Alireza},
  journal={Drones},
  volume={2},
  number={4},
  pages={34},
  year={2018},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@article{kyristsis2016towards,
  title={Towards autonomous modular {UAV} missions: The detection, geo-location and landing paradigm},
  author={Kyristsis, Sarantis and Antonopoulos, Angelos and Chanialakis, Theofilos and Stefanakis, Emmanouel and Linardos, Christos and Tripolitsiotis, Achilles and Partsinevelos, Panagiotis},
  journal={Sensors},
  volume={16},
  number={11},
  pages={1844},
  year={2016},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@inproceedings{acuna2018vision,
  title={Vision-based {UAV} landing on a moving platform in {GPS} denied environments using motion prediction},
  author={Acuna, Raul and Zhang, Ding and Willert, Volker},
  booktitle={2018 Latin American Robotic Symposium, 2018 Brazilian Symposium on Robotics (SBR) and 2018 Workshop on Robotics in Education (WRE)},
  pages={515--521},
  year={2018},
  organization={IEEE}
}
@inproceedings{lee2016vision,
  title={Vision-based {UAV} landing on the moving vehicle},
  author={Lee, Hanseob and Jung, Seokwoo and Shim, David Hyunchul},
  booktitle={2016 International Conference on Unmanned Aircraft Systems (ICUAS)},
  pages={1--7},
  year={2016},
  organization={IEEE}
}
@article{nguyen2018lightdenseyolo,
  title={{LightDenseYOLO}: A fast and accurate marker tracker for autonomous {UAV} landing by visible light camera sensor on drone},
  author={Nguyen, Phong Ha and Arsalan, Muhammad and Koo, Ja Hyung and Naqvi, Rizwan Ali and Truong, Noi Quang and Park, Kang Ryoung},
  journal={Sensors},
  volume={18},
  number={6},
  pages={1703},
  year={2018},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@article{guo2020precision,
  title={Precision Landing Test and Simulation of the Agricultural {UAV} on Apron},
  author={Guo, Yangyang and Guo, Jiaqian and Liu, Chang and Xiong, Hongting and Chai, Lilong and He, Dongjian},
  journal={Sensors},
  volume={20},
  number={12},
  pages={3369},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@article{yuan2018hierarchical,
  title={A hierarchical vision-based localization of rotor unmanned aerial vehicles for autonomous landing},
  author={Yuan, Haiwen and Xiao, Changshi and Xiu, Supu and Zhan, Wenqiang and Ye, Zhenyi and Zhang, Fan and Zhou, Chunhui and Wen, Yuanqiao and Li, Qiliang},
  journal={International Journal of Distributed Sensor Networks},
  volume={14},
  number={9},
  year={2018},
  publisher={SAGE Publications Sage UK: London, England}
}
@article{yang2018hybrid,
  title={Hybrid camera array-based {UAV} auto-landing on moving {UGV} in {GPS}-denied environment},
  author={Yang, Tao and Ren, Qiang and Zhang, Fangbing and Xie, Bolin and Ren, Hailei and Li, Jing and Zhang, Yanning},
  journal={Remote Sensing},
  volume={10},
  number={11},
  pages={1829},
  year={2018},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@inproceedings{olson2011apriltag,
  title={{AprilTag:} A robust and flexible visual fiducial system},
  author={Olson, Edwin},
  booktitle={2011 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={3400--3407},
  year={2011},
  organization={IEEE}
}
@article{berenz2012autonomous,
  title={Autonomous battery management for mobile robots based on risk and gain assessment},
  author={Berenz, Vincent and Tanaka, Fumihide and Suzuki, Kenji},
  journal={Artificial Intelligence Review},
  volume={37},
  number={3},
  pages={217--237},
  year={2012},
  publisher={Springer}
}
@inproceedings{mei2005case,
  title={A case study of mobile robot's energy consumption and conservation techniques},
  author={Mei, Yongguo and Lu, Yung-Hsiang and Hu, Y Charlie and Lee, CS George},
  booktitle={2005 12th International Conference on Advanced Robotics (ICRA)},
  pages={492--497},
  year={2005},
  organization={IEEE}
}
@inproceedings{mei2004energy,
  title={Energy-efficient motion planning for mobile robots},
  author={Mei, Yongguo and Lu, Yung-Hsiang and Hu, Y Charlie and Lee, CS George},
  booktitle={2004 IEEE International Conference on Robotics and Automation (ICRA)},
  volume={5},
  pages={4344--4349},
  year={2004},
  organization={IEEE}
}
@article{mei2006deployment,
  title={Deployment of mobile robots with energy and timing constraints},
  author={Mei, Yongguo and Lu, Yung-Hsiang and Hu, Yu Charlie and Lee, CS George},
  journal={IEEE Transactions on Robotics},
  volume={22},
  number={3},
  pages={507--522},
  year={2006},
  publisher={IEEE}
}
@article{morales2009power,
  title={Power consumption modeling of skid-steer tracked mobile robots on rigid terrain},
  author={Morales, Jesus and Martinez, Jorge L and Mandow, Anthony and Garc{\'\i}a-Cerezo, Alfonso J and Pedraza, Salvador},
  journal={IEEE Transactions on Robotics},
  volume={25},
  number={5},
  pages={1098--1108},
  year={2009},
  publisher={IEEE}
}
@article{seewald2019coarse,
  title={Coarse-Grained Computation-Oriented Energy Modeling for Heterogeneous Parallel Embedded Systems},
  author={Seewald, Adam and Schultz, Ulrik Pagh and Ebeid, Emad and Midtiby, Henrik Skov},
  journal={International Journal of Parallel Programming},
  pages={1--22},
  year={2019},
  publisher={Springer}
}
@inproceedings{nardi2015introducing,
  title={Introducing {SLAMBench}, a performance and accuracy benchmarking methodology for SLAM},
  author={Nardi, Luigi and Bodin, Bruno and Zia, M Zeeshan and Mawer, John and Nisbet, Andy and Kelly, Paul HJ and Davison, Andrew J and Luj{\'a}n, Mikel and O'Boyle, Michael FP and Riley, Graham and others},
  booktitle={2015 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={5783--5790},
  year={2015},
  organization={IEEE}
}
@article{sadrpour2013mission,
  title={Mission Energy Prediction for Unmanned Ground Vehicles Using Real-time Measurements and Prior Knowledge},
  author={Sadrpour, Amir and Jin, Jionghua and Ulsoy, A Galip},
  journal={Journal of Field Robotics},
  volume={30},
  number={3},
  pages={399--414},
  year={2013},
  publisher={Wiley Online Library}
}
@inproceedings{sadrpour2013experimental,
  title={Experimental validation of mission energy prediction model for unmanned ground vehicles},
  author={Sadrpour, Amir and Jin, Judy and Ulsoy, A Galip},
  booktitle={2013 American Control Conference},
  pages={5960--5965},
  year={2013},
  organization={IEEE}
}

@misc{NfoldMarker,
    title = "{N-fold marker tracker repository}",
    author="{Henrik Skov Midtiby}",
    year = "2015",
    howpublished = "\url{https://github.com/henrikmidtiby/MarkerLocator}"
}

@misc{ArUco_marker,
    title = "{Detection of {ArUco} markers}",
    author="{OpenCV}",
    year = "2020",
    howpublished = "\url{https://docs.opencv.org/3.4/d5/dae/tutorial_aruco_detection.html}"
}

@misc{Dynamic_pressure_NASA,
    title = "{Dynamic pressure {(NASA)}}",
    author="{NASA}",
    year = "2020",
    howpublished = "\url{https://www.grc.nasa.gov/WWW/K-12/airplane/dynpress.html}"
}

@book{anderson2010fundamentals,
  title={Fundamentals of aerodynamics},
  author={Anderson Jr, John David},
  year={2010},
  publisher={Tata McGraw-Hill Education}
}

@article{yolov3,
  title={{YOLOv3}: An Incremental Improvement},
  author={Redmon, Joseph and Farhadi, Ali},
  journal = {arXiv},
  year={2018}
}

@misc{bjelonicYolo2018,
  author = {Marko Bjelonic},
  title = {{YOLO ROS}: Real-Time Object Detection for {ROS}},
  howpublished = {\url{https://github.com/leggedrobotics/darknet_ros}},
  year = {2016--2018},
}

\end{filecontents}

\begin{document}





\title{Energy-Aware Design of Vision-Based\\Autonomous Tracking and Landing of a UAV} 
%\title{Energy-Sensitive Vision-Based Autonomous\\Tracking and Landing of a UAV} 

\author{
\IEEEauthorblockN{Georgios Zamanakos, Adam Seewald, Henrik Skov Midtiby, and Ulrik Pagh Schultz}
\IEEEauthorblockA{SDU UAS Center, M{\ae}rsk Mc-Kinney M{\o}ller Institute, University of Southern Denmark\\
Contact email: ups@mmmi.sdu.dk
}}

%% make the title area
\maketitle


\begin{abstract}


In this paper, we present the design and evaluation of a
vision-based algorithm for autonomous tracking and landing on a moving platform 
in varying environmental conditions. %We use
%
We use an energy-aware approach, where the design of the algorithm as a whole is based on
an evaluation of the energy consumption and Quality of Service (QoS) of each computational component.
%
We evaluate the approach based on an agricultural use case where
the moving platform is tracked using 
a landing marker %robust to occlusions 
and the YOLOv3-tiny CNN is used to detect ground-based hazards. We
perform all computations onboard using an NVIDIA Jetson Nano and analyse the impact on the flight time by profiling the energy consumption of the
landing marker detection algorithm and YOLOv3-tiny CNN. Experiments are conducted in Gazebo simulation using an energy modeling tool to measure the computational energy cost as a function of QoS. Our experiments test the energy efficiency and robustness of our system in various dynamic wind disturbances. We show that the landing marker detection algorithm can be run at the highest QoS with only a marginal energy overhead whereas adapting the QoS level of YOLOv3-tiny CNN results in a considerable power saving for the system as a whole. The
power saving is significant for a system executing on a fixed-wing UAV. %but only marginal if executing on a standard multirotor UAV.

\end{abstract}

% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:introduction}

Unmanned Aerial Vehicles (UAVs) are increasingly used for applications
such as monitoring, surveillance, transportation,
and agriculture. %~\cite{costa2012use,salami2014uav}.
%
%One of the major constraints of such applications is their limited level of autonomy due to battery limitations.
Extending the flying time of a UAV is normally done by having it land
in order to replace or charge the battery before continuing the
mission. 
%Performing landings autonomously can however be challenging
%depending on the environment and whether the landing platform is
%stationary or mobile. 
%Moreover, relying solely on the availability of a GPS signal for
%autonomous precision landing is not considered safe, since GPS signals
%can be temporarily lost or even tampered with. As an alternative, 
%
Standard GPS is however insufficient for precision landing, and 
GPS signals can be temporarily lost or even tampered with. 
%In this paper 
We investigate the use of a 
vision-based autonomous landing system and evaluate its robustness
towards environmental conditions such as visual disturbances and wind.

Extension of the flight time can be achieved by using
\emph{energy-aware design of algorithms}, reducing energy consumption
by reducing the Quality of Service (QoS) where it has little impact on overall system performance. 
Energy-costly computations such as computer vision are adapted by
selecting the desired quality of service to match the available
energy~\cite{seewald2020mechanical}. By combining energy-aware algorithm design with autonomous landing
capabilities, we aim to increase the total availability of the UAV to
perform operations, by extending the flight time and using autonomous
recharging when needed.


The main contribution of this paper concerns the experimental study of
a robust vision-based algorithm for autonomous
tracking and landing in varying environmental conditions. The
algorithms are executed on an NVIDIA Jetson Nano companion computer
controlling a simulated drone. The vision-based tracking and landing
algorithms provide novel capabilities in terms of tolerance to visual
disturbance and varying environmental conditions such as wind.
%
Our experiments are based on an agricultural use-case where a
multirotor UAV performs visual identification of ground-based hazards
while tracking and landing on a moving platform.
%




%%%%%%%%%%%%%%%%%%%%%%
\section{State of the Art}
\label{sec:state-of-the-art}

\newcommand{\Xcite}[1]{}

%Vision-based autonomous landing on a marker has been extensively
%studied by many researchers. Key distinctions include the type of the marker along with the computer vision algorithms used to detect it and whether the marker is on a moving platform or not.

%One of the first experiments with
Vision-based autonomous landing has used landing markers
 in an ``H''-shape
pattern; % (similar to ones found on a helipad)~\cite{saripalli2002vision};
landing markers inspired by QR code~\cite{yuan2018hierarchical};
ArUco markers~\cite{lee2012autonomous}\Xcite{yang2018hybrid}, AprilTag markers~\cite{araar2017vision}\Xcite{kyristsis2016towards,feng2018autonomous,olson2011apriltag}; and special-pattern black and white markers~\cite{nguyen2018lightdenseyolo}\Xcite{falanga2017vision} and color markers~\cite{chen2016system}\Xcite{lee2016vision}.  
An onboard implementation of the computer vision algorithms to detect a moving platform has been demonstrated using CPU~\cite{chen2016system}\Xcite{lee2016vision,falanga2017vision} and GPU~\cite{yang2018hybrid}\Xcite{nguyen2018lightdenseyolo,kyristsis2016towards}.
% 
Wind conditions are generally kept stable, i.e.,
a constant wind speed of \SI{5}{\m \per \s} has been used as an external disturbance in a
simulation environment~\cite{feng2018autonomous}. 
%
Concerning %estimation of the UAV's position,
position estimation for landing,
similar to our approach a Kalman Filter or Extended Kalman Filter (EKF) has been used for the
estimation~\cite{saripalli2003landing}\Xcite{araar2017vision,feng2018autonomous,
  falanga2017vision}, as well as a velocity observer algorithm based on calculating the actual
moving distance of the moving platform over a period of time~\cite{yang2018hybrid}.

Energy modeling of computer vision algorithms was investigated in
SLAMBench~\cite{nardi2015introducing}, a framework that automatically evaluates
%Simultaneous Localisation and Mapping (SLAM) 
algorithm configuration alternatives
for energy efficiency. 
%
%Other approaches to energy modeling, such 
Mission-based energy
models~\cite{sadrpour2013experimental}\Xcite{sadrpour2013mission} have 
focused mostly on ground-based autonomous vehicles rather than UAVs. 
The relation between motion and energy in a
robot has been investigated~\cite{morales2009power}, but without accounting for the energy required for computation.
%
Energy modeling of mobile robots~\cite{mei2006deployment}\Xcite{mei2005case, mei2004energy} has provided
the basis for our concept of %modeling computation for
energy-aware algorithm design. Indeed, the approach employed in this paper has evolved
from an energy-efficient motion planning technique~\cite{mei2004energy}, a design strategy that allows accounting for
motion and computations separately~\cite{mei2005case}, to obtain an
energy-efficient robot deployment algorithm~\cite{mei2006deployment}.
%
%To account for the eventuality of battery shortage, a
%battery management mission-based dynamic recharge approach using a set of recharge stations along with
%self-docking capable robots has been proposed~\cite{berenz2012autonomous}. 
%

In this work we propose a new marker pattern robust to occlusions and suitable for a realistic outdoor case. Since the GPU cannot normally run different algorithms simultaneously, we utilize the CPU for detecting the landing marker and the GPU for a CNN to detect ground hazards. By doing so, a different QoS can be chosen for each algorithm. We use \powprof{}, a generic energy
modeling tool~\cite{seewald2019coarse}, %. This tool enables measuring
to measure the energy impact of different configurations of the ROS-based system
implementing the agricultural use-case.
The \powprof{} tool is part of the TeamPlay toolchain, which aims to make tradeoffs between energy and other non-functional properties accessible to the developer. 
In this paper, we present
extensions to \powprof{} that facilitates the initial exploration of
the energy usage of ROS-based systems.



% ORIGINAL TEXT BY ADAM (START)
%To account for the energy modeling of computer vision algorithms, we
%considered the work previously carried by Nardi
%et~al.~\cite{nardi2015introducing}. The authors present SLAMBench, a
%framework that investigates Simultaneous Localisation and Mapping (SLAM) algorithms configuration alternatives
%for energy efficiency. In our work, we use \powprof{}, a generic energy
%modeling tool~\cite{seewald2019coarse}. This tool enables measuring
%the energy impact of different configurations of the ROS-based system
%implementing the agricultural use-case.
%The \powprof{} tool is part of the TeamPlay toolchain, which aims to make tradeoffs between energy and other non-functional properties accessible to the developer. 
%In this paper, we present
%extensions to \powprof{} that facilitates the initial exploration of
%the energy usage of complex ROS-based systems.
%
%Other approaches to energy modeling, such as the mission-based energy
%models studied by Sadrpour et~al.~\cite{sadrpour2013experimental,
%  sadrpour2013mission}, focus mostly on ground-based autonomous
%vehicles instead of the UAVs. Morales et al.~\cite{morales2009power}
%extensively investigated the relation between motion and energy in a
%robot, but do not account for the energy required for computation.
%%
%Energy modeling of mobile robots as carried by Mei et
%al.~\cite{mei2006deployment, mei2005case, mei2004energy} has provided
%the ground for the concept of modeling computation for
%energy-sensitive algorithm design. Indeed, the approach employed in this paper has evolved
%from an energy-efficient motion planning technique
%in~\cite{mei2004energy}, a design strategy that allows accounting for
%motion and computations separately in~\cite{mei2005case}, to an
%energy-efficient deployment algorithm in~\cite{mei2006deployment}.
%
%The battery in our system is considered in the context of a drone
%being able to perform its mission while accounting for the eventuality
%of a battery shortage; to this end, we investigated the approach
%presented by Berenz et~al.~\cite{berenz2012autonomous}, where a
%battery management mission-based dynamic recharge approach is
%presented. A set of recharge stations are used, along with
%self-docking capable robots. Our approach similarly allows landing on
%a moving platform for recharging, which is in the context of this
%paper considered in the proximity of the drone. The actual landing
%is handled by the proposed algorithm, and we also account for the
%energy required for executing this algorithm during landing.
% ORIGINAL TEXT BY ADAM (END)

%Taking into account varying environmental conditions and unpredictable
%movements of the platform to land on is relevant for the use of
%landing in outdoor, mobile scenarios.  Regarding wind conditions, \cite{feng2018autonomous} used
%a constant wind speed of \SI{5}{\m \per \s} as an external disturbance in a
%simulation environment. Nevertheless, a fluctuation in the wind's
%magnitude and direction is likely to happen in realistic cases.
%%
%Concerning estimation of the drone's position,
%similar to our approach a Kalman Filter or Extended Kalman Filter (EKF) has been used for the
%estimation~\cite{saripalli2003landing,araar2017vision,feng2018autonomous,
%  falanga2017vision}, whereas \cite{yang2018hybrid}
%constructed a velocity observer algorithm by calculating the actual
%moving distance of the moving platform over a period of time.

%Taking into account varying environmental conditions and unpredictable
%movements of the platform to land on is relevant for the use of
%landing in outdoor, mobile scenarios.  Regarding wind conditions, an
%AprilTag marker was used by Feng et~al.~\cite{feng2018autonomous} with
%a constant wind speed of \SI{5}{\m \per \s} as an external disturbance in a
%simulation environment. Nevertheless, a fluctuation in the wind's
%magnitude and direction is likely to happen in realistic cases.
%%
%Concerning estimation of the drone's position,
%similar to our approach a Kalman Filter or Extended Kalman Filter (EKF) has been used for the
%estimation~\cite{araar2017vision,feng2018autonomous,
%  falanga2017vision}, whereas Yang et~al.~\cite{yang2018hybrid}
%constructed a velocity observer algorithm by calculating the actual
%moving distance of the moving platform over a period of time.

\section{Energy-Aware System Design} 
%\section{Energy-Sensitive Mission Deployment} 
\label{sec:approach}

%\subsection{Overall approach}

The energy-aware design approach is a mission-oriented concept that adjusts
the computations to the mission being performed while taking into
account energy requirements. %, including energy consumed by actuation,
%computation, and the presence of a limited power source. Specifically,
%In our %agricultural use-case, the concept is employed to 
Here, we profile and adapt the
computationally heavy algorithms performing autonomous tracking,
landing, and hazard detection. This adaptation is energy-aware
in the sense that QoS parameters are adapted
to enable the mission to be completed at the highest possible QoS
level that does not significantly impact the available energy budget. Tradeoffs
between QoS parameters are thus be performed as part of the system design, i.e., trading
the robustness towards wind during landing for precision of hazard
detection.

Energy-aware design using \powprof{} relies on empirical
experiments to measure the actual power consumption on the robot
hardware~\cite{seewald2019coarse}.
%
In this paper, we focus on 
profiling the energy usage of the companion computer,
which from the point of view of energy consumption
can be studied independently from the specific drone it is mounted in.
%
The developer must specify the maximum and minimum QoS level for
each algorithm running on the system.  During mission execution the
levels are statically defined: automatic, dynamic adaptation %during different
%phases of the mission 
is 
considered future work.

The developer executes the system to empirically determine the
power consumption in one of two ways:
%
\begin{enumerate}
\item Automatically using \powprof{} to control the experiment
  execution~\cite{seewald2019coarse}. For a ROS-based system, we assume that the algorithms
  are wrapped as ROS nodes, and we require the developer to declare
  the QoS parameters. % using a ROS configuration. %We use a configuration
  %file in a key-value pair format which is then 
  The QoS parameters are interpreted by
  \powprof{}, iterating through all possible
  combinations and empirically sampling the energy consumption of each
  combination of QoS parameters. Once all combinations have been
  iterated through, \powprof{} automatically combines the energy
  consumption data into a complete model.
\item Semi-automatically using \powprof{} to sample energy and combine
  the results of all experiments, but allowing the developer to
  control all aspects of the experiment execution. This new approach %and is described in more detail later in this section. Basically, 
  uses a dedicated ROS node interfaced to \powprof{}, enabling
  the developer to start/stop sampling in a given
  configuration by publishing on a topic. Once all experiments have been completed, \powprof{} is
  invoked to combine the energy consumption data into
  a complete model.

\end{enumerate}
%
Regardless of the approach, \powprof{} builds a model mapping
QoS to total system energy consumption. Coarse-grained sampling is
employed to reduce the number of experiments, and missing values are
automatically inferred from the others by the means of a multivariate
linear interpolation.

In the context of this paper, sampling experiments are iterated in a
simulated environment with different configurations.  For example, the
autonomous tracking allows changing the tracking algorithm QoS in
terms of frequency, the landing algorithm in terms of frequency, and
hazard detection QoS in terms of frequency.
%


%% \subsection{Semi-automatic energy profiling}
%% \label{sec:semi-automatic}

%% A ROS node has been developed for the purposes of the semi-automatic %energy-sensitive
%% approach used in this paper. This node allows automatic
%% generation of the basic energy models that map time to the
%% instantaneous power consumption. To activate this functionality, the developer simply publishes on a ROS
%% topic to start the model generation, with \powprof{} accounting for
%% the invocation of an asynchronous thread which collects data from the
%% energy sensors. Similarly, the developer publishes on another ROS topic to
%% stop the model generation, while \powprof{} finalizes collecting data from
%% sensors, builds the basic energy model, and stores it for later processing.

%% Once all the basic energy models for the desired QoS ranges have been
%% collected, QoS ranges are specified in a configuration file: the developer defines
%% what QoS configuration corresponds to which basic model
%% (instantaneous power consumption as a function of time). Running
%% \powprof{} using this configuration file as a parameter generates the
%% complete model that maps QoS to energy consumption.

%%%%%%%%%%%%%%%%%%%%
\section{Vision-Based Tracking and Landing}
\label{sec:landing}

%The vision-based autonomous tracking and landing can be split into
%four main sub-problems: detection of the moving platform, navigation,
%guidance, and control of the UAV. 
%From an energy-aware design
%approach, our focus is on the computer vision algorithms used to
%detect the moving platform and the parameterization by a QoS
%influencing energy consumption and performance. %, as described in
%Section~\ref{sec:Detection}. 
%Furthermore, a Kalman Filter is implemented
%%the navigation block is
%%designed 
%to increase the robustness and overall performance of the
%system. 
%A model of
%dynamically changing wind disturbances is analysed and described in
%Section~\ref{sec:WindDisturbances}, allowing the system to be tested
%in a more realistic simulation.

\begin{figure}
\centering
\includegraphics[scale=0.26]{n-fold-for-landing3.png}
\caption{Left: the landing marker. Middle: the kernel that is constructed to detect the n-fold marker (the white regions on the kernel and marker correspond to the real values and the black regions to the imaginary values, the grey area on the kernel is of no concern).
Top-right: detection under occlusion on the tip of the n-fold marker.
Bottom-right: detected under occlusion of a sector of the n-fold marker.}
\label{fig:LandingMarker}
\end{figure}

%\subsection{Detection of the moving platform}
%\label{sec:Detection}

To mark the moving platform a special pattern is used,
consisting of an n-fold marker~\cite{NfoldMarker} and three
ArUco markers~\cite{ArUco_marker} with different ids. This pattern
is referred to as the \emph{landing marker} and can be seen in
Figure~\ref{fig:LandingMarker}. The n-fold marker is primarily used to
detect the moving platform from a high altitude, while the ArUco
markers are used as extra landmarks in case the marker is partially
visible in the image frame.


%To evaluate the computer vision algorithm for detecting the landing
%marker, real images of \SI{640}{}$\times$\SI{480}{} pixel size were captured with
%an Intel RealSense D435 camera. In the Gazebo simulation a color camera
%with the same distortion coefficients as the Intel camera is used to
%output a \SI{640}{}$\times$\SI{480}{} pixel image at 10 frames per second (fps).

To extract the pixel coordinates of the tip (center) of the n-fold marker, a
kernel size of \SI{13}{}$\times$\SI{13}{} pixels consisting of a real and imaginary
part is created. We present a larger kernel size for visualisation purposes in Figure~\ref{fig:LandingMarker}.   For every pixel in the image, a convolution is
performed with this kernel and the magnitude of the convolution is
stored. 
%The pixel with the highest magnitude is considered as a
%candidate tip of the n-fold marker. 
For the pixel with the highest magnitude only, 
%an
%estimation of the orientation/phase of the marker is made and 
an
overall normalized quality score \emph{q} between \SI{0.0}{} and \SI{1.0}{} is calculated. If
the score is above a threshold then the pixel is
accepted as the tip of the n-fold marker. 
%If an n-fold marker is
%detected, the result will be the pixel coordinates of the tip of the
%n-fold marker along with its orientation/phase.
%
An adaptive kernel selection function is implemented to ensure the selection of a
proper minimum kernel size \emph{k} based on a threshold quality score value \emph{q} thus balancing between the computationally expensive convolution process and the effective marker detection. The marker detection results under two different occlusion cases can be seen in Figure~\ref{fig:LandingMarker} for an image size of \SI{640}{}$\times$\SI{480}{} pixel.
%
To detect the ArUco markers the standard OpenCV library is used. %and if
%any ArUco markers are detected, their central pixel coordinates and
%pose are stored.

%Since a convolution is a computationally expensive process, an increase
%in the kernel size would also increase the computation time and
%therefore the energy consumption. However, a higher computation time and energy consumption
%is preferred over a non-detection of the n-fold marker. To balance
%between energy consumption and effective marker detection, an adaptive
%kernel selection function is created to ensure the selection of a
%proper kernel size \emph{k} based on a threshold quality score value \emph{q}. The marker detection results under two different occlusion cases can be seen in Figure~\ref{fig:NfoldOcclusions}. Distance \emph{d} denotes the distance of the marker from the camera.

%Based on a desired quality \emph{q}, the proper kernel size \emph{k} is selected. It is seen that an occlusion on the tip of the n-fold marker results in a significant increase in the selected kernel size.

%In Figure~\ref{fig:NfoldOcclusions} an Intel RealSense D435 Depth camera is used
%to capture the image and measure also the distance \emph{d} from the n-fold
%marker. Based on a desired quality \emph{q}, the proper kernel size \emph{k} is
%selected. It is seen that an occlusion on the tip of the n-fold marker
%results in a significant increase in the selected kernel size.

%\begin{figure}[t]
%\centering
%\includegraphics[scale=0.25]{nfold_occlusions.png}
%\caption{Detection of the landing marker under different
%  occlusions. On the left an occlusion on the tip of the n-fold marker
%  and on the right an occlusion on a sector of the n-fold marker.}
%\label{fig:NfoldOcclusions}
%\end{figure}

%
% 
%To convert the pixel coordinates into a real-world relative position \emph{[X, Y, Z]} the PX4 flight controller's onboard sensor measurements are utilized. For the \emph{Z} component, the altitude of the UAV from the flight controller's EKF is used. To obtain the \emph{X, Y} components, the IMU and barometric sensor measurements are used to project the image plane down to the ground plane. Then a perspective homography matrix is calculated between the two planes, the image plane and the ground plane, thus transforming the pixel coordinates into a relative \emph{X, Y} position in meters. For ArUco markers only, an offset vector in the x, y axis is added depending on the distance of each ArUco marker from the tip of the n-fold marker. 
%It is assumed that this vector is prior known. The mean measurements from the detected n-fold and/or ArUco markers are used to determine the \emph{X, Y} relative position of the landing marker.

To convert the pixel coordinates into a real-world relative position
the PX4 flight controller's onboard sensor measurements are utilized.
The altitude of the UAV is obtained from the flight controller's EKF,
whereas the IMU and barometric sensor measurements are used to project
the image plane down to the ground plane. A perspective homography
matrix is calculated between the two planes to transform the
pixel coordinates into a relative position in meters. For ArUco
markers an offset vector is added depending on the distance of
each ArUco marker from the tip of the n-fold marker.

%The next step is to convert these pixel coordinates into a real-world
%relative position \emph{[X, Y, Z]} according to a local coordinate frame. The
%origin of the local coordinate frame \emph{[0, 0, 0]} is defined as the
%center of the landing marker and alignment according to the North,
%East, Down (\emph{NED}) frame.
%%
%The available sensor measurements and sensor fusion algorithms from
%the flight controller are used in this process. The PX4 flight
%controller outputs through mavlink messages the altitude and the attitude of the UAV (roll, pitch, yaw). For the \emph{Z} component,
%the altitude of the UAV from the flight controller's EKF is used. To
%obtain the \emph{X, Y} components, an algorithm is constructed to convert
%pixel coordinates into real world \emph{X, Y} coordinates in meters:
%
%\begin{enumerate}
%
%\item The pose of the camera in UAV's \emph{BODY} frame is calculated by 
%utilizing the roll and pitch IMU data.
%
%\item The normalized coordinates of the four image corners, according to the 
%camera's horizontal, vertical field of view and the camera's pose from step 1, 
%are calculated.
%
%\item The coordinates of the four image corners (in meters), with respect to 
%the UAV's \emph{BODY} frame, are determined by using the normalised coordinates from 
%step 2 along with the UAV's altitude. The result is a projection plane of the 
%image corners on the ground.
%
%\item The perspective homography matrix is calculated between the two planes, 
%the image plane and the world plane from step 3.
%
%\item The homography matrix from step 4 is used to convert the pixel coordinates 
%of the tip of the n-fold marker from the image plane, into  coordinates (in meters) 
%in the UAV's \emph{BODY} frame.
%
%\item The coordinates from step 5 are in respect to the UAV's \emph{BODY} frame. 
%To convert them into the UAV's \emph{NED} frame, the yaw IMU data from the flight 
%controller is used. 
%
%\item The coordinates from step 6, are converted from UAV's \emph{NED} frame 
%into the landing site's local coordinate frame.
%
%\item For ArUco markers only, an offset vector in the x, y axis is added depending 
%on the distance of each ArUco marker from the tip of the n-fold marker. 
%It is assumed that this vector is prior known.
%
%\end{enumerate}
%%
%The mean measurements from the detected n-fold and/or ArUco markers
%are used to determine the position and orientation of the landing
%marker. The result is an \emph{[X, Y, Z]} relative position of the UAV from
%the moving platform along with the yaw orientation of the landing
%marker.

The tracking algorithm uses an EKF to provide an accurate
prediction for the position of the moving platform at any given
time. This prediction allow images to be processed at different fps
according to a desired QoS. Furthermore, the overall robustness of the
system is increased in case the moving platform is not detected in
every image frame. A velocity estimator for the moving platform is
also implemented as a part of the tracking algorithm.


%\subsection{Navigation}
%\label{sec:Navigation}
%
%The purpose of the navigation block is to provide an accurate
%prediction for the state of the UAV at any given time. Such a prediction allow us to process images at different fps
%according to a desired QoS. Furthermore, the overall robustness of the
%system is increased in case the moving platform is not detected in
%every image frame. A velocity estimator for the moving platform is
%also being implemented as a part of the navigation block.

%The variables of interest that describe the state of the UAV are:
%%
%\begin{itemize}
%
%\item The relative position of the UAV from the moving platform, 
%$x_k\in\mathbb{R}^2$.
%
%\item The attitude of the UAV \emph{[roll, pitch, yaw]}, obtained from the flight controller's IMUs.
%
%\item the velocity of the UAV, $\dot{x}_k$, in \emph{NED} frame, obtained from 
%the flight controller's EKF.
%
%\item the acceleration of the UAV, $\ddot{x}_k$, in \emph{NED} frame, obtained by differentiating the velocities.
%
%\end{itemize}
%%
%The altitude, attitude, velocity and acceleration variables of the
%UAV's state are obtained from the flight controller's onboard sensors
%and already implemented sensor fusion algorithms (EKF). To fuse those
%state variables with the obtained position measurements from
%Section~\ref{sec:Detection}, a Kalman Filter is implemented. The
%measurements from the flight controller are used in the prediction
%step.
%
%Prediction Step:\begin{subequations}
%\begin{align} 
%\hat{x}_{k} & = F \cdot \hat{x}_{k-1} + G_{k} \cdot u_{k}, \\ 
%P_{k} & = F \cdot P_{k-1} \cdot F^\top + Q_{k},
%\end{align}\end{subequations}
%where 
%$u_{k} = \begin{bmatrix}\dot{x}_k & \ddot{x}_k & \dot{x}_{k}^l\end{bmatrix}^\top$ is its control with $x_k,x_k^l$ being the positions of the UAV and the moving platform respectively.
%Given $I:=I_2$, a \SI{2}{}$\times$\SI{2}{} identity matrix, the matrix $F=I$, the noise covariance matrix $Q_k=\Delta t_k\cdot\sigma^2_{\textrm{IMU}}\cdot I$, and $\sigma^2_{\textrm{IMU}}\in\mathbb{R}$ is the variance of the velocities retrieved from the flight controller's data.
%Further, \(\Delta t_k\) is the time interval the flight controller output the data (usually around \SI{33}{\ms}). 
%The input matrix is given by:
%\begin{align*}
%G_{k} & = \begin{bmatrix}
%    \Delta t_k & 0 & \frac{\Delta t_k^{2}}{2} & 0 & -\Delta t_k & 0\\ 
%    0 & \Delta t_k & 0 & \frac{\Delta t_k^{2}}{2} &0 & -\Delta t_k\end{bmatrix},
%\end{align*}
%
%In the correction step, the observed measurements from the downward
%looking camera are used to correct and update the estimated
%position of the UAV $\hat{x}_k$. However due to the computation time
%needed to detect the landing marker, the incoming measurement is
%delayed by a time \(\Delta t_k^{\text{obs}}\in\mathbb{R}_{\geq 0}\). Physically, let us define the displacement $\tilde{x}_k^{\text{obs}}\in\mathbb{R}$ as
%\begin{equation}
%  \tilde{x}_k^{\text{obs}}:=\bar{\dot{x}}_k\,\Delta t_k^{\text{obs}}, %lol why not->|{\hat{x}_t-\hat{x}_v}|\text{ with }v:=t-d_t^{\text{obs}}, seems you loose the amplitude of the velocity???
%\end{equation}
%where $\bar{\dot{x}}_k\in\mathbb{R}$ is the mean velocity measured empirically. Such displacement is later employed in the observed measurements. 
%
%Correction Step:
%\begin{subequations}
%  \begin{align}
%    %e_{t} = x^{\text{obs}}_t+\tilde{x}_t - H\cdot\hat{x}_{t} \\<-not necessary
%    %S_{t} = H\cdot P_{t}\cdot H^T + R_{t} \\<-easier...
%    K_{k} &= P_{k}\cdot H^T(H\cdot P_{k}\cdot H^T + R_{k})^{-1}, \\
%    \hat{x}_{k}^f &= \hat{x}_{k} + K_{k}(x^{\text{obs}}_k+\tilde{x}^{\text{obs}}_k - \hat{x}_{k}), \\ %<- TBH not entirely sure why there as there is no system but directly estimate (you would compare the system with the measurement at this step) but ok; if it works then I am fine with it :) [just confirm it does]
%    P_{k}^f &= (I - K_{k}\cdot H) P_{k},
%  \end{align}
%\end{subequations}
%where $H=I$, and $x_k^{\text{obs}}$ is the observed position at time $k$. Let us further define $R_k:=\sigma^{2}_{\text{obs}}\cdot I$ where $\sigma^{2}_{\text{obs}}\in\mathbb{R}$ is the variance of the velocities estimated by the PX4's EKF. 
%
%The values of the estimated state $\hat{x}_{k}^f$, and error covariance matrix $P_{k}^f$, are used as input to the next iteration of the prediction step $\hat{x}_{k+1},P_{k+1}$.
%
%A velocity estimator is also constructed to determine the magnitude
%and direction of the moving platform's velocity vector. 
%The magnitude
%is calculated by differentiating two sequential positions 
%of the tractor, obtained by detecting the n-fold marker from the 
%vertically facing camera as explained in section \ref{sec:Detection}  
%and taking into consideration the
%UAV's \emph{NED} velocities according to the PX4's EKF. A low-pass
%filter is used to provide a smooth estimation of the velocity's
%magnitude by filtering out high frequency noise. High frequency noise
%can be caused by oscillations of the UAV along with a fast update rate
%in the landing marker detection algorithm.
%
%In the agricultural use-case the moving platform is likely to change
%its direction up to 180 degrees. Furthermore, it is assumed that the
%moving platform is a nonholonomic system, like a tractor. To
%compensate for sudden turns, the moving platform's yaw orientation
%will be taken into account. Based on the velocity's magnitude and moving platform's yaw orientation, the
%moving platform's velocity (\(\dot{x}_k^l\)) in \emph{NED} frame can be obtained.


%% \subsection{Wind disturbances}
%% \label{sec:WindDisturbances}
%% %The exact and accurate estimation of the applied wind forces on a body
%% %is a complex matter studied by the field of fluid dynamics. We use a
%% %simplified approach, as follows. 
%% We assume that the wind will be
%% applied on an area of \SI{0.09}{\m^2}. Such an area is emulating a UAV
%% with extra payload attached on its frame. Two different wind speeds of
%% \SI{8}{\m \per \s} and \SI{12}{\m \per \s} are used to calculate the applied wind forces
%% on that area. The wind forces are considered to be applied on the
%% center of gravity of the UAV with direction parallel to the ground. The magnitude of the applied force, corresponding to a certain wind
%% speed is calculated by the following equations
%% \cite{Dynamic_pressure_NASA,anderson2010fundamentals}:
%% \begin{equation}
%%     \begin{array}{l}
%%          F_{\textrm{init}} = p_{d} \cdot A,\,\,\,
%%          p_{d} = \frac{\varrho \cdot v^2}{2},
%%     \end{array}
%% \end{equation}
%% \noindent where $F_{\text{init}}\in\mathbb{R}$ is the force, \(p_{d}\in\mathbb{R}\) is the dynamic pressure, \(A\in\mathbb{R}\) is
%% the area of the applied pressure, \(\varrho\in\mathbb{R}\) is the density of the
%% air (around \SI{1.2}{\kg \per \m^3}), and \(v\in\mathbb{R}\) is the wind velocity in \SI{}{\m \per \s}. By solving the above equations the applied force on the UAV is found
%% to be \SI{3.45}{\newton} for an \SI{8}{\m \per \s} wind speed, and 
%% \SI{7.76}{\newton} for a \SI{12}{\m \per \s}
%% wind speed. The wind forces are applied on the UAV in Gazebo
%% simulation, to test the performance of the whole system. The direction of the wind force remains constant while its magnitude changes every \SI{5.5}{\second} between -10\% and +20\% of the initial wind force of either \SI{3.45}{\newton} or \SI{7.76}{\newton}. For a UAV altitude of \SI{6.0}{\meter} and below, the wind force decreases linearly. This model is used for all simulated tests.


%
%This is
%done by constructing a program that applies the wind forces on the
%virtual UAV. 

%At first the wind's direction vector is defined as
%$w=\begin{bmatrix}0.8 & 0.2\end{bmatrix}^\top$ according to Gazebo's \emph{(x, y)} axes and an initial wind force ($F_{\text{init}}$) of either \SI{3.45}{\newton} or \SI{7.76}{\newton} is chosen. An update cycle of \SI{5.5}{\second} is chosen between two different applied forces. A random float number between \SI{0.9}{} and \SI{1.2}{} is chosen at every update cycle. This number is defined as $r\in\mathbb{R}$. The applied wind force is calculated as $\hat{F}=w\cdot F\cdot r$. For a UAV altitude of \SI{6.0}{\meter} and above, $F=F_{\text{init}}$ while for a UAV altitude of \SI{6.0}{\meter} and below, the wind force decreases as:
%   $F=F_{\text{init}}\cdot h/ 6$ where $h$ is the altitude. This model is created to simulate different scaling in the wind's magnitude and is used for all simulated tests.

%To simulate a wind pattern the wind direction and magnitude must be
%defined. The wind direction is assumed to remain the same for the
%whole duration of the experiments. That direction vector is defined as
%$w=\begin{bmatrix}0.8 & 0.2\end{bmatrix}^\top$ according to Gazebo's \emph{(x, y)} axes.
%%
%The magnitude of the wind is calculated as follows: 
%
%\begin{itemize}
%    \item An initial wind force of either \SI{3.45}{\newton} or \SI{7.76}{\newton} is chosen.
%    
%    \item An update cycle of \SI{5.5}{\second} is chosen between two different applied forces.
%    
%    \item A random float number between \SI{0.9}{} and \SI{1.2}{} is chosen at every update 
%    cycle. This number is defined as $r\in\mathbb{R}$.
%    
%    \item The applied wind force is calculated as $\hat{F}=w\cdot F\cdot r$ :
%
%\begin{itemize}
%    \item For a UAV altitude of \SI{6.0}{\meter} and above, $F=F_{\text{init}}$. 
%    \item For a UAV altitude of \SI{6.0}{\meter} and below, the wind force decreases as:
%   $F=F_{\text{init}}\cdot h/ 6$ where $h$ is the altitude.
%   
%   \item For a UAV altitude of \SI{3}{\meter} and below, $F = 0.5$. 
%\end{itemize}
%\end{itemize}
%%
%This model is created to simulate different scaling in the wind's magnitude and is used for all simulated tests.

%%%%%%%%%%%%%%%%%%%%
\section{Evaluation}
\label{sec:experimental}
% Georgios: I think we can ommit that first parapragh

%We evaluate our approach in terms of the quality of the overall
%functionality and the energy efficiency of the algorithms. All
%algorithms are executed on an embedded companion computer interfaced
%to a simulation running on a standard computer.

\subsection{Use-case: agricultural safety}

We evaluate our approach based on a simulated use-case where a multirotor UAV identifies
hazardous objects around a moving platform, and lands on the
moving platform to recharge. A simulated field is created in Gazebo with randomly placed objects as seen in Figure~\ref{fig:Gazebo}.
%No communication link is considered between the moving platform and the UAV
%and no GNSS positioning is assumed to be available. The system can thus be
%considered as a fallback for fault-tolerance.

Object detection and classification of cars, humans, tractors and cows is performed by feeding the input
image from the downward facing camera into a \emph{YOLOv3-tiny} CNN~\cite{yolov3} implemented in ROS~\cite{bjelonicYolo2018}. 
%Four
%different classes are selected: cars, humans, tractors, and
%cows. 
%Based on the CNN's predictions and onboard
%sensors, the UAV maps the detected objects onto a 2D map. 
%The performance of \emph{YOLOv3-tiny} on pre-trained weights was not satisfactory and therefore 
A custom dataset based on the Gazebo models was created consisting of 1200 images, 300
for each class. We trained the CNN by its default training
parameters for 5000 epochs, for an input image size of \SI{416}{}$\times$\SI{416}{}
pixels.

\begin{figure}[t]
\centering
\includegraphics[scale=0.2]{gazebo_scalled_gamma_corrected.png}
\caption{On the left, a top view of the Gazebo scene. 
On the right, a view of the UAV attempting a landing on the moving platform.}
\label{fig:Gazebo}
\end{figure}





% were initially used but the
%performance on detecting objects from a downward facing camera was
%not satisfactory. Since a dataset for detecting the above four classes
%from a top view was not available, we created an artificial dataset
%based on the Gazebo models. The dataset consists of 1200 images, 300
%for each class. We trained the \emph{YOLOv3-tiny} by its default training
%parameters for 5000 epochs, for an input image size of \SI{416}{}$\times$\SI{416}{}
%pixels.

%Pre-trained weights for \emph{YOLOv3-tiny}, were initially used but the
%performance on detecting objects from a downward facing camera was
%not satisfactory. Since a dataset for detecting the above four classes
%from a top view was not available, we created an artificial dataset
%based on the Gazebo models. The dataset consists of 1200 images, 300
%for each class. We trained the \emph{YOLOv3-tiny} by its default training
%parameters for 5000 epochs, for an input image size of \SI{416}{}$\times$\SI{416}{}
%pixels.

We assume a windy environment where %, as follows. %that the wind will be
%applied on an area of \SI{0.09}{\m^2}. Such an area is emulating a UAV
%with extra payload attached on its frame. Two different wind speeds of
%\SI{8}{\m \per \s} and \SI{12}{\m \per \s} are used to calculate the applied wind forces
%on that area. 
wind forces are applied on the
center of gravity of the UAV with direction parallel to the ground. The magnitude of the applied force %corresponds to certain 
is calculated from the speed~\cite{Dynamic_pressure_NASA,anderson2010fundamentals}:
%% \begin{equation}
%%     \begin{array}{l}
%%          F_{\textrm{init}} = p_{d} \cdot A,\,\,\,
%%          p_{d} = \frac{\varrho \cdot v^2}{2},
%%     \end{array}
%% \end{equation}
%% \noindent where $F_{\text{init}}\in\mathbb{R}$ is the force, \(p_{d}\in\mathbb{R}\) is the dynamic pressure, \(A\in\mathbb{R}\) is
%% the area of the applied pressure, \(\varrho\in\mathbb{R}\) is the density of the
%% air (around \SI{1.2}{\kg \per \m^3}), and \(v\in\mathbb{R}\) is the wind velocity in \SI{}{\m \per \s}. %By solving the above equations 
The applied force on the UAV is found
to be \SI{3.45}{\newton} for an \SI{8}{\m \per \s} wind speed, and 
\SI{7.76}{\newton} for a \SI{12}{\m \per \s}
wind speed. %The wind forces are applied on the UAV in Gazebo
%simulation, to test the performance of the whole system. 
The direction of the wind force remains constant while its magnitude changes every \SI{5.5}{\second} between -10\% and +20\% of the initial wind force. %of either \SI{3.45}{\newton} or \SI{7.76}{\newton}. 
For a UAV altitude of \SI{6.0}{\meter} and below, the wind force decreases linearly. This model is used for all simulated tests.


\subsection{Experimental setup}

All experiments are performed in Gazebo %simulation under 
using Ubuntu~18.04 and ROS Melodic on a standard
%i7-8550U 1.8~GHz (4.0~GHz Boost), 8~GB DDR4
laptop. The \emph{PX4} Software In The Loop (SITL) Firmware v1.10.2 is used as the flight
controller and the \emph{IRIS} quadcopter is used as the UAV platform. A
downward facing RGB camera is placed on the UAV providing a
\SI{640}{}$\times$\SI{480}{} pixel image at 10~fps. An NVIDIA Jetson Nano
with Ubuntu~18.04 and ROS Melodic is used as the UAV's companion
computer. 
%
Energy profiling is performed directly on the NVIDIA Jetson
  Nano using \powprof{} as outlined in Section~\ref{sec:approach}.
  
%Experiments are conducted to evaluate the energy consumption and QoS of \emph{tracking} and \emph{landing} modes. 
%
%Two groups of experiments are conducted. The first group evaluates the
%energy consumption and QoS of the \emph{tracking} mode and the second group
%evaluates the energy consumption and QoS of the \emph{landing} mode. 
%For
%both groups, the experiments start with the tractor moving at a
%constant speed of \SI{0.3}{\meter\per\second}, according to a square path similar to that of a
%plowing tractor, and the UAV taking off and hovering at an altitude of
%\SI{25}{\meter}. After reaching the desired altitude, the UAV starts searching
%for the landing marker in the image frame. 
%Once the landing marker is
%detected, the UAV commences its actions.
%
In \emph{tracking} mode the UAV follows the moving platform at a fixed
altitude and maps the ground environment.
%use its downward facing camera to map the environment,
In \emph{landing} mode the UAV follows the moving platform and
gradually lowers its altitude until it lands on it. Both \emph{tracking} and
\emph{landing} modes are evaluated for energy consumption and QoS under cases of no wind, \SI{8}{\meter \per \second} and \SI{12}{\meter \per \second} wind disturbances. %as analysed in Section~\ref{sec:WindDisturbances}.


\subsection{Results}

%The first group of experiments was conducted to test the \emph{tracking} mode and evaluate its 
%energy efficiency and QoS. 
For the energy 
evaluation of \emph{tracking} mode, eight tests were executed for different fps rates 
for the \emph{YOLOv3-tiny} ROS node (\emph{YRN}) (4fps, 1fps, 0.5fps, 0.1fps) and 
the \emph{landing marker} detection ROS node (\emph{LRN}) (10fps, 0.5fps). as 
%
%seen in Figure~\ref{fig:PowerDuringTracking}. 
For a 4fps update rate
for \emph{YRN}
%\emph{YOLOv3-tiny},
a power consumption of \SI{6.30}{\watt} is observed while
for a 1fps and 0.1fps update rates, the power consumption drops to
\SI{4,8}{\watt} and \SI{3.9}{\watt} accordingly. By reducing the update rate for \emph{LRN},
%\emph{landing marker} detection,
from 10fps to 0.5fps, a further power
saving of \SI{0.15}{\watt}- \SI{0.19}{\watt} is achieved.



For the QoS evaluation of \emph{tracking} mode, twelve tests were executed for different 
fps rates for the \emph{YRN}
%\emph{YOLOv3-tiny} ROS node
 (4fps, 0.1fps) and for 
the \emph{LRN}
%\emph{landing marker} detection ROS node
 (10fps, 0.5fps) under 
three different cases of wind disturbances. 
%
The QoS is determined as the number of correctly detected objects (objects detected within a distance of \SI{2}{\meter} from their actual position and classified correctly). 
%
%An object is considered to be correctly detected if it is detected within
%a distance of \SI{2}{\meter} from its actual position and classified with
%the correct class. 
The detection results can be seen in
Figure~\ref{fig:NCorrectObjectDetections}. The best results were
obtained for a high fps update rate in \emph{YRN} %\emph{YOLOv3-tiny}
 and \emph{LRN},
% \emph{landing marker} detection,
under no wind disturbances, where 28 out of the 32
objects were detected. %This is expected since at a 
Higher fps rates imply that each object is viewed more times as the UAV moves, but %. Nevertheless, 
for the same high fps values with a wind speed of \SI{12}{\meter \per \second} only 18 out of 32 objects were detected. This difference is due to the UAV %has to 
being at a certain roll and pitch angle to balance itself against the wind, which causes some objects to be either outside the downward facing camera's field of view, or to be viewed from an angle which the CNN has not been trained for.


%The second group of experiments was conducted to test the \emph{landing} mode
%and evaluate its energy efficiency and QoS.
For the energy evaluation of \emph{landing} mode, eight tests were executed for different 
fps rates for the \emph{LRN}
%\emph{landing marker} detection ROS node 
(10fps, 2fps, 1fps, 0.5fps) under two different cases. 
In the first case, the kernel size remains fixed at 
\SI{22}{}$\times$\SI{22}{} pixels and in the second case the standard adaptive selection 
kernel size algorithm is used. 
%
%The landing time is also taken under consideration as seen in
%Figure~\ref{fig:PowerDuringLanding}. 
The largest observed difference in the
power consumption is \SI{0.14}{\watt} and is observed between the 0.5fps and
the 10fps update rate for the \emph{LRN}.
%\emph{landing marker} detection.
However
the landing time is reduced by \SI{30}{\second} when using the 10fps update
rate. The adaptive kernel size in most cases outperforms the fixed
kernel size by \SI{0.11}{\watt}. 
%Furthermore the adaptive kernel size can
%compensate for marker occlusions which will increase the overall
%robustness of the system.

For the QoS evaluation of \emph{landing} mode, twelve tests were executed for different 
fps rates for the \emph{LRN}
%\emph{landing marker} detection ROS node 
(10fps, 2fps, 1fps, 0.5fps) under three different cases of wind
disturbances. The QoS is determined as the  mean squared error (MSE) 
between the predicted and actual position of the moving platform.
Four different altitude bins were used as seen in 
Figure~\ref{fig:PositionErrorDuringLanding}. 
A large MSE of around \SI{3}{\square\meter} is observed for an altitude 
greater than \SI{20}{\meter} for a 0.5fps rate, while an MSE close to 
zero is observed for an altitude of less than \SI{5}{\meter} for an 
update rate of 10fps.  Wind disturbances do not seem to 
have an influence on the MSE. We believe the larger MSE for the \emph{y} 
coordinates compared to the \emph{x} coordinates is caused by a 
sudden change of the moving platform's direction on the \emph{y} axis.

%% \begin{figure}[t]
%%   \centering
%%   \includegraphics{data_visualization/PowerDetection.pdf}
%%   \caption{Power consumption during tracking mode. \emph{YRN} fps:
%%     0=0.1fps, 1=0.5fps, 2=1fps, 3=4fps.}
%%   \label{fig:PowerDuringTracking}
%% \end{figure}
  
\begin{figure}[t]
  \centering
  \includegraphics{data_visualization/QoSDetection.pdf}
  \caption{Number of correctly detected objects
  under different conditions. 
  Grey color denotes a 0.1fps and black color denotes a 4fps 
  update rate in \emph{YRN}. 
  Circles denote a 0.5fps and triangles a 10fps update 
  rate in the \emph{LRN}.}
  \label{fig:NCorrectObjectDetections}
\end{figure}

%% \begin{figure}[t]
%% \centering
%% \includegraphics{data_visualization/PowerLanding.pdf}
%% \caption{Power consumption during landing mode. 
%% The black circles denote a fixed kernel size while the grey circles denote an adaptive kernel size.}
%% \label{fig:PowerDuringLanding}
%% \end{figure}

\begin{figure}[t]
\centering
\includegraphics{data_visualization/QoSLanding.pdf}
\caption{Position error during landing and how it depends 
on the marker detection rate at different altitudes and wind disturbances.
Circles denote errors in the x direction and triangles 
errors in the y direction.}
\label{fig:PositionErrorDuringLanding}
\end{figure}

\subsection{Discussion}

The experiments show that both \emph{tracking} and \emph{landing} modes are
supported by the system, in a simulated environment with a moving
platform and random wind conditions. Moreover, the performance of both
modes is sensitive to the QoS, with a high success rate of both modes
at high QoS levels, and significantly lower performance at lower QoS
levels.

The potential energy savings from having an energy-sensitive algorithm
that can adapt its QoS by changing the fps values for the \emph{YRN} and \emph{LRN}
%\emph{YOLOv3-tiny} and \emph{landing marker} ROS nodes 
should be seen
in relation to the total energy consumption of the UAV. As a concrete
example, consider a DJI Phantom 4 multirotor and a Sky-Watch Cumulus
fixed-wing (the fixed-wing would need to circle while tracking and 
would need VTOL capabilities to land). We estimate\footnote{From information on the respective
  product pages regarding battery capacity and maximal flight time.}
that the Phantom uses roughly \SI{140}{\watt} while cruising whereas the Cumulus
uses roughly \SI{40}{\watt} while cruising. The maximal saving gained from
changing the \emph{YRN}
%\emph{YOLOv3-tiny} 
rate is 
$\SI{6.30}{\watt}-\SI{3.9}{\watt}=\SI{2.4}{\watt}$ whereas the
maximal saving gained from changing the \emph{LRN}
%\emph{landing marker} 
rate is
\SI{0.2}{\watt}. For the Cumulus, there is thus a 6.5\% potential energy
saving, whereas the potential energy saving only is 1.9\% for the
Phantom. For the Cumulus this saving is considered large enough to
significantly impact the flying time of the drone, with a total energy
saving of \SI{23.4}{\kilo \joule}. For the Cumulus, the potential saving from adapting
the \emph{LRN}
%\emph{landing marker} 
QoS is however only 0.5\%. 

For the tracking mode, changing the \emph{LRN}
%\emph{landing marker} 
rate
provided a minor saving of only \SI{0.14}{\watt}, but at the cost of increased
landing time. Therefore, although the higher-QoS computer vision
algorithm is marginally more expensive by \SI{0.14}{\watt}, the UAV will
overall save energy due to a reduced flight time.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion and Future Work}
\label{sec:conclusion}

In this paper we presented %a robust, energy-sensitive, 
the energy-aware design of a vision-based
algorithm for autonomous tracking and landing in varying environmental
conditions, based on experimentally executing all the necessary algorithms on 
the NVIDIA Jetson Nano companion computer.
%
Our experiments show that the proposed computer vision algorithms for 
detecting the moving platform can be run at the highest QoS level with only a marginal
energy overhead, whereas adapting the QoS level of the CNN %\emph{YRN}
%\emph{YOLOv3-tiny} CNN
results in a considerable power saving for the system as a whole, 
with a significant power saving for a system executing on a fixed-wing UAV. %
%but only marginal if executing on a multirotor UAV.

In terms of future work, we are interested in automatically adapting
the QoS level to the available battery, and in testing this approach
on a physical drone.

%% acknowledgement
%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgment}

This work is supported and partly funded by the European Union’s
Horizon2020 research and innovation program under grant agreement
No.~779882 (TeamPlay).

\bibliographystyle{IEEEtran}
\bibliography{\jobname} 
\vspace{1ex}

\end{document}
